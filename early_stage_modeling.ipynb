{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Early-stage modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By: Jared Berry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for data preparation/EDA\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from scipy import stats\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up & data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure working directory is set appropriately - change as needed\n",
    "os.chdir('C:\\\\Users\\\\jared\\\\Documents\\\\data_science\\\\school\\\\georgetown\\\\capstone\\\\Passive-Stock-Fund-Optimization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets into memory\n",
    "yahoo = pd.read_csv('stock_price_until_2019_04_28.csv')\n",
    "drvd = pd.read_csv('moving-avg-momentum.csv')\n",
    "simfin = pd.read_csv('simfin\\daily_simfin.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dimensions\n",
    "print(\"Yahoo! Finance data has {} observations and {} features.\".format(yahoo.shape[0], yahoo.shape[1]))\n",
    "print(\"Derived data has {} observations and {} features.\".format(drvd.shape[0], drvd.shape[1]))\n",
    "print(\"Daily SimFin data has {} observations and {} features.\".format(simfin.shape[0], simfin.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check keys\n",
    "print(\"Yahoo! Finance:\")\n",
    "print(yahoo[['Symbol', 'date_of_transaction']].head())\n",
    "print(\"Derived (Yahoo! Finance)\")\n",
    "print(drvd[['Symbol', 'Date']].head())\n",
    "print(\"SimFin:\")\n",
    "print(simfin[['ticker', 'date']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some quick fixes on keys\n",
    "yahoo['ticker'] = yahoo['Symbol']\n",
    "yahoo.drop('Symbol', axis=1, inplace=True)\n",
    "\n",
    "drvd['ticker'] = drvd['Symbol']\n",
    "drvd['date_of_transaction'] = drvd['Date']\n",
    "drvd.drop(['Symbol', 'Date', 'High', 'Low', \n",
    "           'Open', 'Close', 'Volume', 'AdjClose'], \n",
    "          axis=1, inplace=True)\n",
    "\n",
    "simfin['date_of_transaction'] = simfin['date']\n",
    "simfin.drop('date', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the 'train' dataset by merging stock prices and fundamentals; ensure proper sorting and filter on early sample\n",
    "train = pd.merge(yahoo, drvd, on=['ticker', 'date_of_transaction'])\n",
    "train = pd.merge(train, simfin, how='left', on=['ticker', 'date_of_transaction'])\n",
    "\n",
    "train = train.sort_values(['ticker','date_of_transaction'])\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct some aggregate financial ratios from the SimFin data\n",
    "train['eps'] = train['net_income_y'] / train['common_outstanding_basic']\n",
    "train['pe_ratio'] = train['AdjClose'] / train['eps']\n",
    "train['debt_ratio'] = train['total_liabilities'] / train['total_equity']\n",
    "train['debt_to_equity'] = train['total_liabilities'] / train['total_equity']\n",
    "train['roa'] = train['net_income_y'] / train['total_assets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct some additional ticker-level returns features\n",
    "train['open_l1'] = train.groupby('ticker')['Open'].shift(1)\n",
    "train['open_l5'] = train.groupby('ticker')['Open'].shift(5)\n",
    "train['open_l10'] = train.groupby('ticker')['Open'].shift(10)\n",
    "\n",
    "train['return_prev1_open_raw'] = 100*(train['Open'] - train['open_l1'])/train['open_l1']\n",
    "train['return_prev5_open_raw'] = 100*(train['Open'] - train['open_l5'])/train['open_l5']\n",
    "train['return_prev10_open_raw'] = 100*(train['Open'] - train['open_l10'])/train['open_l10']\n",
    "\n",
    "train['close_l1'] = train.groupby('ticker')['AdjClose'].shift(1)\n",
    "train['close_l5'] = train.groupby('ticker')['AdjClose'].shift(5)\n",
    "train['close_l10'] = train.groupby('ticker')['AdjClose'].shift(10)\n",
    "\n",
    "train['return_prev1_close_raw'] = 100*(train['AdjClose'] - train['close_l1'])/train['close_l1']\n",
    "train['return_prev5_close_raw'] = 100*(train['AdjClose'] - train['close_l5'])/train['close_l5']\n",
    "train['return_prev10_close_raw'] = 100*(train['AdjClose'] - train['close_l10'])/train['close_l10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the quarter of pre-SimFin data\n",
    "train = train[train['date_of_transaction'] >= '2011-03-31'].reset_index().drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify ranges\n",
    "n = 1 # n-day ahead return\n",
    "q = 21 # q-day window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At the ticker level, lead the AdjClose column by n-trading days\n",
    "target_gen = train[['ticker', 'date_of_transaction', 'AdjClose']]\n",
    "AdjClose_ahead = target_gen.groupby('ticker')['AdjClose'].shift(-n)\n",
    "AdjClose_ahead.name = 'AdjClose_ahead'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw, month-ahead return is calculated as:\n",
    "$$target_{t,i} = \\frac{AdjClose_{t+n,i} - AdjClose_{t,i}}{AdjClose_{t,i}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_raw = np.array(100*((AdjClose_ahead - train['AdjClose'])/train['AdjClose']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing all of the returns for the next 21 days (month) relative to today\n",
    "aheads = []\n",
    "for i in range(0,n+1):\n",
    "    AdjClose_ahead_i = target_gen.groupby('ticker')['AdjClose'].shift(-i)\n",
    "    aheads.append(np.array(100*((AdjClose_ahead_i - train['AdjClose'])/train['AdjClose'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average, raw returns for all periods within the next month (relative to today) is calculated as:\n",
    "$$target_{t,i} = (\\frac{1}{n})\\sum_{k=1}^n \\frac{AdjClose_{t+k,i} - AdjClose_{t,i}}{AdjClose_{t,i}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cma = np.array(pd.DataFrame(aheads).mean(axis=0, skipna=False).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The w-day moving average of n-day ahead raw returns, relative to today is calculated as:\n",
    "$$target_{t,i,q} = \\frac{AdjClose_{t+n,i} - AdjClose_{t,i}}{AdjClose_{t,i}}, MA(q)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simpler moving-average ahead target?\n",
    "target_gen['returns_ahead'] = 100*((AdjClose_ahead - train['AdjClose'])/train['AdjClose'])\n",
    "target_ma = np.array(target_gen.groupby('ticker')['returns_ahead'].rolling(q).mean()) # NEED TO CHECK THE LAG STRUCTURE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct market-residualized variants (NEED S&P 500 OR CONSTRUCTION OF AN INDEX FROM CURRENT FEATURES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporatory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a feature selection list (THINK ABOUT INFORMING THIS SELECTION WITH SHRINKAGE METHODS, I.E. RIDGE REGRESSION)\n",
    "features = ['High', 'Low', 'Open', 'Close', 'Volume', 'AdjClose', 'Year',\n",
    "            'Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear', 'Pct_Change_Daily',\n",
    "            'Pct_Change_Monthly', 'Pct_Change_Yearly', 'RSI', 'Volatility',\n",
    "            'Yearly_Return_Rank', 'Monthly_Return_Rank', 'Pct_Change_Class',\n",
    "            'Rolling_Yearly_Mean_Positive_Days', 'Rolling_Monthly_Mean_Positive_Days', \n",
    "            'Rolling_Monthly_Mean_Price', 'Rolling_Yearly_Mean_Price',\n",
    "            'open_l1', 'open_l5', 'open_l10', 'close_l1', 'close_l5', 'close_l10',\n",
    "            'return_prev1_open_raw', 'return_prev5_open_raw', 'return_prev10_open_raw',\n",
    "            'return_prev1_close_raw', 'return_prev5_close_raw', 'return_prev10_close_raw',\n",
    "            'pe_ratio', 'debt_ratio', 'debt_to_equity', 'roa'\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select on features to pass to modeling machinery\n",
    "X = train[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of the tickers\n",
    "tickers = train['ticker'].unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set relevant scikit-learn functions/modules\n",
    "\n",
    "# Tests for stationarity \n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Regression models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Classification models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# LightGBM\n",
    "from lightgbm import LGBMClassifier\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# Model selection\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Evaluation\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Wheat Classification notebook \n",
    "def fit_sklearn_classifier(X, y, holdout, ticker, ema_gamma, valid_splits, model, label, param_search={}, export=False,\n",
    "                           panel = False, groups=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Flexible function for fitting any number of sci-kit learn\n",
    "    classifiers, with optional grid search.\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    # Convert to NumPy arrays - store feature names\n",
    "    feature_names = X.columns.tolist()\n",
    "    features = np.array(X)\n",
    "    test_features = np.array(holdout)\n",
    "    labels = y.copy()\n",
    "    \n",
    "    # Compute EMA smoothing of target prior to constructing classes\n",
    "    EMA = 0\n",
    "    gamma_ = ema_gamma\n",
    "    for ti in range(len(labels)):\n",
    "        EMA = gamma_*labels[ti] + (1-gamma_)*EMA\n",
    "        labels[ti] = EMA  \n",
    "        \n",
    "    labels_smoothed = np.where(labels >= 0, 1, 0)    \n",
    "    orig_labels = np.where(y.copy() >= 0, 1, 0)\n",
    "    print(\"\\n{} labels changed by smoothing.\".format(np.sum(labels_smoothed != orig_labels)))\n",
    "\n",
    "    # Compute some baselines\n",
    "    all_pos_acc = np.mean(np.ones(test_features.shape[0], dtype='int') == labels_smoothed)\n",
    "    random_walk_classes = np.array(pd.Series(labels_smoothed).shift(1).tolist()[1:], dtype=\"int\")\n",
    "    rw_acc = np.mean(random_walk_classes == np.array(labels_smoothed[1:], dtype=\"int\"))\n",
    "    print(\"Baseline accuracy is: {}%\".format(100*np.round(all_pos_acc, 2)))\n",
    "    print(\"RW accuracy is {}%\".format(100*np.round(rw_acc, 2)))  \n",
    "    \n",
    "    # Set time-series, cross-validation indices\n",
    "    if panel:\n",
    "        splits = PanelSplit(n_folds=valid_splits, groups=groups)\n",
    "        search_splits = PanelSplit(n_folds=valid_splits, groups=groups)\n",
    "    else:\n",
    "        splits  = TimeSeriesSplit(n_splits=valid_splits).split(features)\n",
    "        search_splits = TimeSeriesSplit(n_splits=valid_splits).split(features)\n",
    "    \n",
    "    # Empty array for feature importances\n",
    "    feature_importance_values = np.zeros(len(feature_names))\n",
    "\n",
    "    # Empty array for out of fold validation predictions\n",
    "    out_of_fold = np.zeros(features.shape[0])\n",
    "    \n",
    "    # Empty array for test predictions\n",
    "    test_predictions = np.zeros(test_features.shape[0])\n",
    "    \n",
    "    # Dictionary of lists for recording validation and training scores\n",
    "    scores = {'precision':[], 'recall':[], 'accuracy':[], 'f1':[]}\n",
    "    \n",
    "    # Perform a grid-search on the provided parameters to determine best options\n",
    "    if param_search:\n",
    "        gridsearch = GridSearchCV(estimator=model(), \n",
    "                                  cv=search_splits,\n",
    "                                  param_grid=param_search)\n",
    "\n",
    "        # Fit to extract best parameters later\n",
    "        gridsearch_model = gridsearch.fit(features, labels_smoothed)\n",
    "\n",
    "    opt_thresholds = []\n",
    "    for train_indices, valid_indices in splits:\n",
    "        # Training data for the fold\n",
    "        train_features, train_labels = features[train_indices], labels_smoothed[train_indices]\n",
    "        #Validation data for the fold\n",
    "        valid_features, valid_labels = features[valid_indices], labels_smoothed[valid_indices]\n",
    "        \n",
    "        expected = valid_labels\n",
    "        \n",
    "        # Generate a model given the optimal parameters established in grid search\n",
    "        if param_search:\n",
    "            estimator = model(**gridsearch_model.best_params_)\n",
    "        else:\n",
    "            estimator = model(**kwargs)\n",
    "            \n",
    "        # Train the estimator\n",
    "        estimator.fit(train_features, train_labels)\n",
    "\n",
    "        # Fit the fitted model on the test set and store positive class probabilities\n",
    "        probs = estimator.predict_proba(valid_features)\n",
    "        pos_probs = [p[1] for p in probs]\n",
    "\n",
    "        # Dynamic classification threshold selection\n",
    "        thresholds = list(np.arange(0.30, 0.90, 0.05))\n",
    "        preds = [[1 if y >= t else 0 for y in pos_probs] for t in thresholds]\n",
    "        scores_by_threshold_ = [metrics.f1_score(valid_labels, p) for p in preds]\n",
    "        opt_thresh_ = thresholds[scores_by_threshold_.index(max(scores_by_threshold_))]\n",
    "        opt_thresholds.append(opt_thresh_)\n",
    "\n",
    "        # Generate class predictions\n",
    "        predicted = [1 if y >= opt_thresh_ else 0 for y in pos_probs]\n",
    "\n",
    "        # Append scores to the tracker\n",
    "        scores['precision'].append(metrics.precision_score(expected, predicted, average=\"weighted\"))\n",
    "        scores['recall'].append(metrics.recall_score(expected, predicted, average=\"weighted\"))\n",
    "        scores['accuracy'].append(metrics.accuracy_score(expected, predicted))\n",
    "        scores['f1'].append(metrics.f1_score(expected, predicted, average=\"weighted\"))\n",
    "        \n",
    "        # Store variable importances \n",
    "        if model in [RandomForestClassifier, GradientBoostingClassifier]:\n",
    "            feature_importance_values += estimator.feature_importances_ / splits.n_splits\n",
    "            \n",
    "    # Properly format feature importances\n",
    "    named_importances = list(zip(feature_names, feature_importance_values))\n",
    "    sorted_importances = sorted(named_importances, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Fit on full sample \n",
    "    if param_search:\n",
    "        estimator = model(**gridsearch_model.best_params_)\n",
    "    else:\n",
    "        estimator = model(**kwargs)\n",
    "        \n",
    "    estimator.fit(features, labels_smoothed)\n",
    "    \n",
    "    # Test on hold out set\n",
    "    out_of_sample_probs = estimator.predict_proba(holdout)\n",
    "    pos_probs = [p[1] for p in out_of_sample_probs]\n",
    "    predicted = [1 if y >= opt_thresh_ else 0 for y in pos_probs]\n",
    "\n",
    "    # Store values for later reporting/use in app\n",
    "    evals = {'precision':np.mean(scores['precision']), \n",
    "             'recall':np.mean(scores['recall']), \n",
    "             'accuracy':np.mean(scores['accuracy']), \n",
    "             'f1':np.mean(scores['f1']),\n",
    "             'probabilities':pos_probs,\n",
    "             'predictions':predicted,\n",
    "             'importances':sorted_importances\n",
    "             }\n",
    "    \n",
    "    # Report\n",
    "    print(\"Build, hyperparameter selection, and validation of {} took {:0.3f} seconds\\n\".format(label, time.time()-start))\n",
    "    print(\"Hyperparameters are as follows:\")\n",
    "    if param_search:\n",
    "        for key in gridsearch_model.best_params_.keys():\n",
    "            print(\"{}: {}\\n\".format(key, gridsearch_model.best_params_[key]))\n",
    "    print(\"Validation scores are as follows:\")\n",
    "    print(pd.DataFrame(scores).mean())\n",
    "    \n",
    "    # Output the evals dictionary\n",
    "    if export:\n",
    "        outpath = \"{}_{}.pickle\".format(label.tolower().replace(\" \", \"_\"), ticker.tolower())\n",
    "        with open(outpath, 'wb') as f:\n",
    "            pickle.dump(evals, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_sklearn_regressor(X, y, holdout, ticker, ema_gamma, valid_splits, model, label, param_search={}, export=False, \n",
    "                                **kwargs):\n",
    "    \n",
    "    start = time.time()\n",
    "    # Convert to NumPy arrays - store feature names\n",
    "    feature_names = X.columns.tolist()\n",
    "    features = np.array(X)\n",
    "    test_features = np.array(holdout)\n",
    "    returns = y.copy()\n",
    "    \n",
    "    # Compute EMA smoothing of target prior to constructing classes\n",
    "    EMA = 0\n",
    "    gamma_ = ema_gamma\n",
    "    for ti in range(len(returns)):\n",
    "        EMA = gamma_*returns[ti] + (1-gamma_)*EMA\n",
    "        returns[ti] = EMA  \n",
    "        \n",
    "    returns_smoothed = returns.copy() \n",
    "\n",
    "    # Set time-series, cross-validation indices\n",
    "    splits  = TimeSeriesSplit(n_splits=valid_splits)\n",
    "    search_splits = TimeSeriesSplit(n_splits=valid_splits).split(features)\n",
    "    \n",
    "    # Empty array for feature importances\n",
    "    feature_importance_values = np.zeros(len(feature_names))\n",
    "\n",
    "    # Empty array for out of fold validation predictions\n",
    "    out_of_fold = np.zeros(features.shape[0])\n",
    "    \n",
    "    # Empty array for test predictions\n",
    "    test_predictions = np.zeros(test_features.shape[0])\n",
    "    \n",
    "    # Dictionary of lists for recording validation and training scores\n",
    "    scores = {'mae':[], 'mse':[], 'r^2':[]}\n",
    "    \n",
    "    # Perform a grid-search on the provided parameters to determine best options\n",
    "    if param_search:\n",
    "        gridsearch = GridSearchCV(estimator=model(), \n",
    "                                  cv=search_splits,\n",
    "                                  param_grid=param_search)\n",
    "\n",
    "        # Fit to extract best parameters later\n",
    "        gridsearch_model = gridsearch.fit(features, returns_smoothed)\n",
    "\n",
    "    opt_thresholds = []\n",
    "    for train_indices, valid_indices in splits.split(features):\n",
    "        # Training data for the fold\n",
    "        train_features, train_returns = features[train_indices], returns_smoothed[train_indices]\n",
    "        #Validation data for the fold\n",
    "        valid_features, valid_returns = features[valid_indices], returns_smoothed[valid_indices]\n",
    "        \n",
    "        expected = valid_returns\n",
    "        \n",
    "        # Generate a model given the optimal parameters established in grid search\n",
    "        if param_search:\n",
    "            estimator = model(**gridsearch_model.best_params_)\n",
    "        else:\n",
    "            estimator = model(**kwargs)\n",
    "            \n",
    "        # Train the estimator\n",
    "        estimator.fit(train_features, train_returns)   \n",
    "        \n",
    "        # Generate predictions\n",
    "        predicted = estimator.predict(valid_features)\n",
    "\n",
    "        # Append scores to the tracker\n",
    "        scores['mae'].append(metrics.mean_absolute_error(expected, predicted))\n",
    "        scores['mse'].append(metrics.mean_squared_error(expected, predicted))\n",
    "        scores['r^2'].append(metrics.r2_score(expected, predicted))\n",
    "        \n",
    "        # Store variable importances \n",
    "        if model in [RandomForestClassifier, GradientBoostingClassifier]:\n",
    "            feature_importance_values += estimator.feature_importances_ / splits.n_splits\n",
    "            \n",
    "    # Properly format feature importances\n",
    "    named_importances = list(zip(feature_names, feature_importance_values))\n",
    "    sorted_importances = sorted(named_importances, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Fit on full sample \n",
    "    if param_search:\n",
    "        estimator = model(**gridsearch_model.best_params_)\n",
    "    else:\n",
    "        estimator = model(**kwargs)\n",
    "        \n",
    "    estimator.fit(features, returns_smoothed)\n",
    "    \n",
    "    # Test on hold out set\n",
    "    predicted = estimator.predict(holdout)    \n",
    "    \n",
    "    # Store values for later reporting/use in app\n",
    "    evals = {'mae':np.mean(scores['mae']), \n",
    "             'mse':np.mean(scores['mse']), \n",
    "             'r^2':np.mean(scores['r^2']),\n",
    "             'predictions':predicted,\n",
    "             'importances':sorted_importances\n",
    "             }\n",
    "    \n",
    "    # Report\n",
    "    print(\"Build, hyperparameter selection, and validation of {} took {:0.3f} seconds\\n\".format(label, time.time()-start))\n",
    "    print(\"Hyperparameters are as follows:\")\n",
    "    if param_search:\n",
    "        for key in gridsearch_model.best_params_.keys():\n",
    "            print(\"{}: {}\\n\".format(key, gridsearch_model.best_params_[key]))\n",
    "    print(\"Validation scores are as follows:\")\n",
    "    print(pd.DataFrame(scores).mean())\n",
    "    \n",
    "    # Output the evals dictionary\n",
    "    if export:\n",
    "        outpath = \"{}_{}.pickle\".format(label.tolower().replace(\" \", \"_\"), ticker.tolower())\n",
    "        with open(outpath, 'wb') as f:\n",
    "            pickle.dump(evals, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_lgbm_classifier(X, y, holdout, ticker, ema_gamma=1, valid_splits=12, export=False):\n",
    "    \n",
    "    # Convert to NumPy arrays - store feature names\n",
    "    feature_names = X.columns.tolist()\n",
    "    features = np.array(X)\n",
    "    test_features = np.array(holdout)\n",
    "    labels = y.copy()\n",
    "    \n",
    "    # Compute EMA smoothing of target prior to constructing classes\n",
    "    EMA = 0\n",
    "    gamma_ = ema_gamma\n",
    "    for ti in range(len(labels)):\n",
    "        EMA = gamma_*labels[ti] + (1-gamma_)*EMA\n",
    "        labels[ti] = EMA  \n",
    "        \n",
    "    labels_smoothed = np.where(labels >= 0, 1, 0)\n",
    "    orig_labels = np.where(y.copy() >= 0, 1, 0)\n",
    "    print(\"\\n{} labels changed by smoothing.\".format(np.sum(labels_smoothed != orig_labels)))\n",
    "\n",
    "    # Compute some baselines\n",
    "    all_pos_acc = np.mean(np.ones(test_features.shape[0], dtype='int') == labels_smoothed)\n",
    "    random_walk_classes = np.array(pd.Series(labels_smoothed).shift(1).tolist()[1:], dtype=\"int\")\n",
    "    rw_acc = np.mean(random_walk_classes == np.array(labels_smoothed[1:], dtype=\"int\"))\n",
    "    print(\"Baseline accuracy is: {}%\".format(100*np.round(all_pos_acc, 2)))\n",
    "    print(\"RW accuracy is {}%\".format(100*np.round(rw_acc, 2)))\n",
    "    \n",
    "    ## pd.DataFrame(list(zip(labels_smoothed, y.copy()))).plot()\n",
    "\n",
    "    # Generate splits\n",
    "    splits = TimeSeriesSplit(n_splits=valid_splits)\n",
    "\n",
    "    # Empty array for feature importances\n",
    "    feature_importance_values = np.zeros(len(feature_names))\n",
    "\n",
    "    # Empty array for out of fold validation predictions\n",
    "    out_of_fold = np.zeros(features.shape[0])\n",
    "    \n",
    "    # Empty array for test predictions\n",
    "    test_predictions = np.zeros(test_features.shape[0])\n",
    "\n",
    "    # Lists for recording validation and training scores\n",
    "    valid_scores = []\n",
    "    train_scores = []\n",
    "    valid_accs = []\n",
    "    train_accs = []\n",
    "    \n",
    "    # Iterate through each fold\n",
    "    for train_indices, valid_indices in splits.split(X): \n",
    "\n",
    "        # Training data for the fold\n",
    "        train_features, train_labels = features[train_indices], labels_smoothed[train_indices]\n",
    "        #Validation data for the fold\n",
    "        valid_features, valid_labels = features[valid_indices], labels_smoothed[valid_indices]\n",
    "\n",
    "        # Create the bst\n",
    "        bst = LGBMClassifier(n_estimators=10000, objective = 'binary', \n",
    "                             class_weight = 'balanced', learning_rate = 0.01,\n",
    "                             max_bin = 25, num_leaves = 25, max_depth = 1,\n",
    "                             reg_alpha = 0.1, reg_lambda = 0.1, \n",
    "                             subsample = 0.8, random_state = 101,\n",
    "                             boosting = 'gbdt'\n",
    "                            )\n",
    "\n",
    "        # Train the bst\n",
    "        bst.fit(train_features, train_labels, eval_metric = ['auc', 'binary_error'],\n",
    "                eval_set = [(valid_features, valid_labels), (train_features, train_labels)],\n",
    "                eval_names = ['valid', 'train'],\n",
    "                early_stopping_rounds = 100, verbose = 0)\n",
    "\n",
    "        # Record the best iteration\n",
    "        best_iteration = bst.best_iteration_\n",
    "\n",
    "        # Record the feature importances\n",
    "        feature_importance_values += bst.feature_importances_ / splits.n_splits\n",
    "        \n",
    "        # Make predictions\n",
    "        test_predictions += bst.predict_proba(test_features, num_iteration = best_iteration)[:, 1] / splits.n_splits\n",
    "\n",
    "        # Record the out of fold predictions\n",
    "        out_of_fold[valid_indices] = bst.predict_proba(valid_features, num_iteration = best_iteration)[:, 1]\n",
    "\n",
    "        # Record the best score\n",
    "        valid_score = bst.best_score_['valid']['auc']\n",
    "        train_score = bst.best_score_['train']['auc']\n",
    "        valid_acc = bst.best_score_['valid']['binary_error']\n",
    "        train_acc = bst.best_score_['train']['binary_error']\n",
    "        \n",
    "        valid_scores.append(valid_score)\n",
    "        train_scores.append(train_score)\n",
    "        valid_accs.append(valid_acc)\n",
    "        train_accs.append(train_acc)\n",
    "        \n",
    "    # Properly format feature importances\n",
    "    named_importances = list(zip(feature_names, feature_importance_values))\n",
    "    sorted_importances = sorted(named_importances, key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "    # Set up an exportable dictionary with results from the model\n",
    "    results = {\n",
    "        'train_auc':train_scores,\n",
    "        'train_acc':train_acc,\n",
    "        'validation_auc':valid_scores,\n",
    "        'validation_accuracy':valid_acc,\n",
    "        'valid_preds':out_of_fold,\n",
    "        'feature_importances':sorted_importances,\n",
    "        'test_predictions':test_predictions\n",
    "    }\n",
    "    \n",
    "    # Output the results dictionary\n",
    "    if export:\n",
    "        outpath = \"lgbc_{}.pickle\".format(ticker.tolower())\n",
    "        with open(outpath, 'wb') as f:\n",
    "            pickle.dump(results, f)\n",
    "    \n",
    "    print(\"Average AUC across {} splits: {}\".format(valid_splits, np.mean(valid_scores)))\n",
    "    print(\"Average accuracy across {} splits: {}%\".format(valid_splits, 100*np.round((1-np.mean(valid_acc)),2)))\n",
    "    for i in range(6):\n",
    "        print(sorted_importances[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_lgbm_regressor(X, y, holdout, ticker, ema_gamma=1, valid_splits=12, export=False):\n",
    "    \n",
    "    # Convert to NumPy arrays - store feature names\n",
    "    feature_names = X.columns.tolist()\n",
    "    features = np.array(X)\n",
    "    test_features = np.array(holdout)\n",
    "    returns = y.copy()\n",
    "    \n",
    "    # Compute EMA smoothing of target prior to constructing classes\n",
    "    EMA = 0\n",
    "    gamma_ = ema_gamma\n",
    "    for ti in range(len(returns)):\n",
    "        EMA = gamma_*returns[ti] + (1-gamma_)*EMA\n",
    "        returns[ti] = EMA  \n",
    "        \n",
    "    returns_smoothed = returns.copy()\n",
    "\n",
    "    # Generate splits\n",
    "    splits = TimeSeriesSplit(n_splits=valid_splits)\n",
    "\n",
    "    # Empty array for feature importances\n",
    "    feature_importance_values = np.zeros(len(feature_names))\n",
    "\n",
    "    # Empty array for out of fold validation predictions\n",
    "    out_of_fold = np.zeros(features.shape[0])\n",
    "    \n",
    "    # Empty array for test predictions\n",
    "    test_predictions = np.zeros(test_features.shape[0])\n",
    "\n",
    "    # Lists for recording validation and training scores\n",
    "    valid_scores = []\n",
    "    train_scores = []\n",
    "    \n",
    "    # Iterate through each fold\n",
    "    for train_indices, valid_indices in splits.split(X): \n",
    "\n",
    "        # Training data for the fold\n",
    "        train_features, train_returns = features[train_indices], returns_smoothed[train_indices]\n",
    "        #Validation data for the fold\n",
    "        valid_features, valid_returns = features[valid_indices], returns_smoothed[valid_indices]\n",
    "\n",
    "        # Create the bst\n",
    "        bst = LGBMRegressor(n_estimators=10000, objective = 'regression', \n",
    "                            learning_rate = 0.01,\n",
    "                            max_bin = 25, num_leaves = 25, max_depth = 1,\n",
    "                            reg_alpha = 0.1, reg_lambda = 0.1, \n",
    "                            subsample = 0.8, random_state = 101,\n",
    "                            boosting = 'gbdt'\n",
    "                            )\n",
    "\n",
    "        # Train the bst\n",
    "        bst.fit(train_features, train_returns, eval_metric = ['l1', 'l2'],\n",
    "                eval_set = [(valid_features, valid_returns), (train_features, train_returns)],\n",
    "                eval_names = ['valid', 'train'],\n",
    "                early_stopping_rounds = 100, verbose = 0)\n",
    "\n",
    "        # Record the best iteration\n",
    "        best_iteration = bst.best_iteration_\n",
    "\n",
    "        # Record the feature importances\n",
    "        feature_importance_values += bst.feature_importances_ / splits.n_splits\n",
    "        \n",
    "        # Make predictions\n",
    "        test_predictions += bst.predict(test_features, num_iteration = best_iteration) / splits.n_splits\n",
    "\n",
    "        # Record the out of fold predictions\n",
    "        out_of_fold[valid_indices] = bst.predict(valid_features, num_iteration = best_iteration)\n",
    "\n",
    "        # Record the best score\n",
    "        valid_score = bst.best_score_['valid']['l1']\n",
    "        train_score = bst.best_score_['train']['l1']\n",
    "        \n",
    "        valid_scores.append(valid_score)\n",
    "        train_scores.append(train_score)\n",
    "        \n",
    "    # Properly format feature importances\n",
    "    named_importances = list(zip(feature_names, feature_importance_values))\n",
    "    sorted_importances = sorted(named_importances, key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "    # Set up an exportable dictionary with results from the model\n",
    "    results = {\n",
    "        'train_auc':train_scores,\n",
    "        'validation_auc':valid_scores,\n",
    "        'valid_preds':out_of_fold,\n",
    "        'feature_importances':sorted_importances,\n",
    "        'test_predictions':test_predictions\n",
    "    }\n",
    "    \n",
    "    # Output the results dictionary\n",
    "    if export:\n",
    "        outpath = \"lgbr_{}.pickle\".format(ticker.tolower())\n",
    "        with open(outpath, 'wb') as f:\n",
    "            pickle.dump(results, f)\n",
    "    \n",
    "    print(\"\\nAverage MAE across {} splits: {}\".format(valid_splits, np.mean(valid_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Panel-level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that there are bound to be a number of systemic considerations that impact the price of a stock at any given point in time, it is prudent to perform and evaluate predictions across the panel of S&P 500 stocks in our sample, which will capture potential linkages between different stocks, and allow us to explore the possibility of using features generated from clustering to group like stocks in the panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create copies for panel-level regressions\n",
    "X_p = X.copy(deep=True)\n",
    "y_p = target_raw.copy()\n",
    "\n",
    "# Indexes of hold-out test data (the 21 days of data preceding the present day)\n",
    "test_idx = np.where(np.isnan(y_p))[0].tolist()\n",
    "\n",
    "# In order to ensure grouping is done properly, remove this data from a ticker-identification set as well\n",
    "ticker_locs = train['ticker'].drop(train.index[test_idx]).reset_index().drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PanelSplit(n_folds, groups, window=False):\n",
    "    \"\"\"\n",
    "    Function to generate time series splits of a panel, provided\n",
    "    a number of folds, and an indexable dataframe to create groups.\n",
    "    \"\"\"\n",
    "    # Generate a list of the indexes of the indexes for each entity in the data\n",
    "    idx_list = [ticker_locs.loc[ticker_locs['ticker'] == t].index.tolist() for t in tickers]\n",
    "    \n",
    "    # Storage for indexes \n",
    "    train_splits = [[] for _ in range(n_folds)]\n",
    "    valid_splits = [[] for _ in range(n_folds)]\n",
    "    \n",
    "    for idx in idx_list:\n",
    "        splits = TimeSeriesSplit(n_splits=n_folds)\n",
    "        fold = 0\n",
    "        for train_indices, valid_indices in splits.split(idx):\n",
    "            train_splits[fold] += np.array(idx)[train_indices].tolist()\n",
    "            valid_splits[fold] += np.array(idx)[valid_indices].tolist()\n",
    "            fold += 1\n",
    "            \n",
    "    panel_splits = list(zip(train_splits, valid_splits))\n",
    "    \n",
    "    return(panel_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple feature-scaling - for now, replace missings with 0 (i.e. the mean of a normalized feature)\n",
    "X_p = X_p.groupby(['Year', 'Month', 'Day']).apply(lambda x: (x - np.mean(x))/np.std(x)).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove hold-out test data\n",
    "y_p = np.delete(y_p, test_idx)\n",
    "X_p_holdout = X_p.loc[X_p.index[test_idx]]\n",
    "X_p = X_p.drop(X_p.index[test_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_p_smoothed = np.zeros(y_p.shape[0])\n",
    "for t in tickers:\n",
    "    idx = ticker_locs.loc[ticker_locs['ticker'] == t].index.tolist()\n",
    "    y_to_smooth = y_p[idx]\n",
    "    \n",
    "    # Compute EMA smoothing of target within ticker\n",
    "    EMA = 0\n",
    "    gamma_ = 0.75\n",
    "    for ti in range(len(y_to_smooth)):\n",
    "        EMA = gamma_*y_to_smooth[ti] + (1-gamma_)*EMA\n",
    "        y_to_smooth[ti] = EMA\n",
    "        \n",
    "    y_p_smoothed[idx] = y_to_smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and evaluate - gamma MUST be 1 here\n",
    "## fit_lgbm_classifier(X_p, y_p_smoothed, X_p_holdout, ticker=\"\", ema_gamma=1, valid_splits=12, export=False)\n",
    "fit_sklearn_classifier(X_p, y_p_smoothed, X_p_holdout, ticker=\"\", ema_gamma=1, valid_splits=12, model=KNeighborsClassifier,\n",
    "                       label='kNN Classifier', param_search = {}, export=False, n_jobs=-1,\n",
    "                       panel = True, groups=ticker_locs\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ticker-level "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the heart of this analysis is a time-series prediction problem. As such, it is prudent to explore running models for each individual stock. We can envision averaging the results of both modeling approaches to incorporate the contribution of both into a final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, t in enumerate(tickers[:5]):\n",
    "    \n",
    "    # Pull only feature/target data for the relevant stocker\n",
    "    X_t = X.loc[train['ticker'] == t,:]\n",
    "    y_t = target_raw[train['ticker'] == t]\n",
    "    \n",
    "    # Indexes of hold-out test data (the 21 days of data preceding the present day)\n",
    "    test_idx = np.where(np.isnan(y_t))[0].tolist()\n",
    "    \n",
    "    # Simple feature-scaling - for now, replace missings with 0 (i.e. the mean of a normalized feature)\n",
    "    X_t = X_t.apply(lambda x: (x - np.mean(x))/np.std(x)).fillna(0)\n",
    "    \n",
    "    # Remove hold-out test data\n",
    "    y_t = np.delete(y_t, test_idx)\n",
    "    X_t_holdout = X_t.loc[X_t.index[test_idx]]\n",
    "    X_t = X_t.drop(X_t.index[test_idx])\n",
    "    \n",
    "    # Fit and evaluate\n",
    "    fit_lgbm_classifier(X_t, y_t, X_t_holdout, ticker=t, ema_gamma=1, valid_splits=12, export=False)\n",
    "    ## fit_lgbm_regressor(X_t, y_t, X_t_holdout, ticker=t, ema_gamma=0.1, valid_splits=12, export=False)\n",
    "    ## fit_sklearn_classifier(X_t, y_t, X_t_holdout, ticker=t, ema_gamma=1, valid_splits=12, model=KNeighborsClassifier,\n",
    "    ##                        label='kNN Classifier', param_search = {'n_neighbors':[2,4,6]}, export=False, n_jobs=-1\n",
    "    ##                      )\n",
    "    ## fit_sklearn_regressor(X_t, y_t, X_t_holdout, ticker=t, ema_gamma=0.1, valid_splits=12, model=KNeighborsRegressor,\n",
    "    ##                        label='kNN Regressor', param_search = {}, export=False, n_jobs=-1\n",
    "    ##                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's essential to benchmark the performance of our models against some work-horse time-series forecasting models. Regardless of differential performance (to a point), what these models lack is the ability for us to understand *what* makes a model perform the way it does, and doesn't directly serve our objective of providing users with *relative probabilities* of performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_arima(y, gamma, p, d, q):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_prophet(y, gamma):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1; t = tickers[i]#for i, t in enumerate(tickers[:5]):\n",
    "    \n",
    "# Pull only feature/target data for the relevant stocker\n",
    "X_t = X.loc[train['ticker'] == t,:]\n",
    "y_t = target_raw[train['ticker'] == t]\n",
    "\n",
    "# Indexes of hold-out test data (the 21 days of data preceding the present day)\n",
    "test_idx = np.where(np.isnan(y_t))[0].tolist()\n",
    "\n",
    "# Simple feature-scaling - for now, replace missings with 0 (i.e. the mean of a normalized feature)\n",
    "X_t = X_t.apply(lambda x: (x - np.mean(x))/np.std(x)).fillna(0)\n",
    "\n",
    "# Remove hold-out test data\n",
    "y_t = np.delete(y_t, test_idx)\n",
    "X_t_holdout = X_t.loc[X_t.index[test_idx]]\n",
    "X_t = X_t.drop(X_t.index[test_idx])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
