{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Early-stage modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for data preparation/EDA\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from scipy import stats\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up & data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure working directory is set appropriately - change as needed\n",
    "os.chdir('C:\\\\Users\\\\jared\\\\Documents\\\\data_science\\\\school\\\\georgetown\\\\capstone\\\\Passive-Stock-Fund-Optimization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets into memory\n",
    "yahoo = pd.read_csv('stock_price_until_2019_04_28.csv')\n",
    "drvd = pd.read_csv('moving-avg-momentum.csv')\n",
    "simfin = pd.read_csv('simfin\\daily_simfin.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yahoo! Finance data has 1014554 observations and 22 features.\n",
      "Derived data has 1014554 observations and 20 features.\n",
      "Daily SimFin data has 1509516 observations and 53 features.\n"
     ]
    }
   ],
   "source": [
    "# Check dimensions\n",
    "print(\"Yahoo! Finance data has {} observations and {} features.\".format(yahoo.shape[0], yahoo.shape[1]))\n",
    "print(\"Derived data has {} observations and {} features.\".format(drvd.shape[0], drvd.shape[1]))\n",
    "print(\"Daily SimFin data has {} observations and {} features.\".format(simfin.shape[0], simfin.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yahoo! Finance:\n",
      "  Symbol date_of_transaction\n",
      "0    ABT          2011-01-03\n",
      "1    ABT          2011-01-04\n",
      "2    ABT          2011-01-05\n",
      "3    ABT          2011-01-06\n",
      "4    ABT          2011-01-07\n",
      "Derived (Yahoo! Finance)\n",
      "  Symbol        Date\n",
      "0      A  2011-01-03\n",
      "1      A  2011-01-04\n",
      "2      A  2011-01-05\n",
      "3      A  2011-01-06\n",
      "4      A  2011-01-07\n",
      "SimFin:\n",
      "  ticker        date\n",
      "0    MMM  2011-03-31\n",
      "1    MMM  2011-04-01\n",
      "2    MMM  2011-04-02\n",
      "3    MMM  2011-04-03\n",
      "4    MMM  2011-04-04\n"
     ]
    }
   ],
   "source": [
    "# Check keys\n",
    "print(\"Yahoo! Finance:\")\n",
    "print(yahoo[['Symbol', 'date_of_transaction']].head())\n",
    "print(\"Derived (Yahoo! Finance)\")\n",
    "print(drvd[['Symbol', 'Date']].head())\n",
    "print(\"SimFin:\")\n",
    "print(simfin[['ticker', 'date']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some quick fixes on keys\n",
    "yahoo['ticker'] = yahoo['Symbol']\n",
    "yahoo.drop('Symbol', axis=1, inplace=True)\n",
    "\n",
    "drvd['ticker'] = drvd['Symbol']\n",
    "drvd['date_of_transaction'] = drvd['Date']\n",
    "drvd.drop(['Symbol', 'Date', 'High', 'Low', \n",
    "           'Open', 'Close', 'Volume', 'AdjClose'], \n",
    "          axis=1, inplace=True)\n",
    "\n",
    "simfin['date_of_transaction'] = simfin['date']\n",
    "simfin.drop('date', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sno</th>\n",
       "      <th>date_of_transaction</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>AdjClose</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>...</th>\n",
       "      <th>total_assets</th>\n",
       "      <th>total_current_assets</th>\n",
       "      <th>total_current_liabilities</th>\n",
       "      <th>total_equity</th>\n",
       "      <th>total_liabilities</th>\n",
       "      <th>total_liabilities_equity</th>\n",
       "      <th>total_noncurrent_assets</th>\n",
       "      <th>total_noncurrent_liabilities</th>\n",
       "      <th>common_outstanding_basic</th>\n",
       "      <th>common_outstanding_diluted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22334</th>\n",
       "      <td>22334</td>\n",
       "      <td>2011-01-03</td>\n",
       "      <td>30.143061</td>\n",
       "      <td>29.620888</td>\n",
       "      <td>29.728184</td>\n",
       "      <td>29.957081</td>\n",
       "      <td>4994000.0</td>\n",
       "      <td>27.591616</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22335</th>\n",
       "      <td>22335</td>\n",
       "      <td>2011-01-04</td>\n",
       "      <td>30.114449</td>\n",
       "      <td>29.456366</td>\n",
       "      <td>30.035765</td>\n",
       "      <td>29.678112</td>\n",
       "      <td>5017200.0</td>\n",
       "      <td>27.334681</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22336</th>\n",
       "      <td>22336</td>\n",
       "      <td>2011-01-05</td>\n",
       "      <td>29.849785</td>\n",
       "      <td>29.327610</td>\n",
       "      <td>29.513592</td>\n",
       "      <td>29.613733</td>\n",
       "      <td>4519000.0</td>\n",
       "      <td>27.275387</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22337</th>\n",
       "      <td>22337</td>\n",
       "      <td>2011-01-06</td>\n",
       "      <td>29.928469</td>\n",
       "      <td>29.477825</td>\n",
       "      <td>29.592276</td>\n",
       "      <td>29.670958</td>\n",
       "      <td>4699000.0</td>\n",
       "      <td>27.328091</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22338</th>\n",
       "      <td>22338</td>\n",
       "      <td>2011-01-07</td>\n",
       "      <td>29.899857</td>\n",
       "      <td>29.356224</td>\n",
       "      <td>29.699572</td>\n",
       "      <td>29.771101</td>\n",
       "      <td>3810900.0</td>\n",
       "      <td>27.420322</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 84 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sno date_of_transaction       High        Low       Open      Close  \\\n",
       "22334  22334          2011-01-03  30.143061  29.620888  29.728184  29.957081   \n",
       "22335  22335          2011-01-04  30.114449  29.456366  30.035765  29.678112   \n",
       "22336  22336          2011-01-05  29.849785  29.327610  29.513592  29.613733   \n",
       "22337  22337          2011-01-06  29.928469  29.477825  29.592276  29.670958   \n",
       "22338  22338          2011-01-07  29.899857  29.356224  29.699572  29.771101   \n",
       "\n",
       "          Volume   AdjClose  Year  Month  ...  total_assets  \\\n",
       "22334  4994000.0  27.591616  2011      1  ...           NaN   \n",
       "22335  5017200.0  27.334681  2011      1  ...           NaN   \n",
       "22336  4519000.0  27.275387  2011      1  ...           NaN   \n",
       "22337  4699000.0  27.328091  2011      1  ...           NaN   \n",
       "22338  3810900.0  27.420322  2011      1  ...           NaN   \n",
       "\n",
       "       total_current_assets  total_current_liabilities  total_equity  \\\n",
       "22334                   NaN                        NaN           NaN   \n",
       "22335                   NaN                        NaN           NaN   \n",
       "22336                   NaN                        NaN           NaN   \n",
       "22337                   NaN                        NaN           NaN   \n",
       "22338                   NaN                        NaN           NaN   \n",
       "\n",
       "       total_liabilities  total_liabilities_equity  total_noncurrent_assets  \\\n",
       "22334                NaN                       NaN                      NaN   \n",
       "22335                NaN                       NaN                      NaN   \n",
       "22336                NaN                       NaN                      NaN   \n",
       "22337                NaN                       NaN                      NaN   \n",
       "22338                NaN                       NaN                      NaN   \n",
       "\n",
       "       total_noncurrent_liabilities  common_outstanding_basic  \\\n",
       "22334                           NaN                       NaN   \n",
       "22335                           NaN                       NaN   \n",
       "22336                           NaN                       NaN   \n",
       "22337                           NaN                       NaN   \n",
       "22338                           NaN                       NaN   \n",
       "\n",
       "       common_outstanding_diluted  \n",
       "22334                         NaN  \n",
       "22335                         NaN  \n",
       "22336                         NaN  \n",
       "22337                         NaN  \n",
       "22338                         NaN  \n",
       "\n",
       "[5 rows x 84 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct the 'train' dataset by merging stock prices and fundamentals; ensure proper sorting and filter on early sample\n",
    "train = pd.merge(yahoo, drvd, on=['ticker', 'date_of_transaction'])\n",
    "train = pd.merge(train, simfin, how='left', on=['ticker', 'date_of_transaction'])\n",
    "\n",
    "train = train.sort_values(['ticker','date_of_transaction'])\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct some aggregate financial ratios from the SimFin data\n",
    "train['eps'] = train['net_income_y'] / train['common_outstanding_basic']\n",
    "train['pe_ratio'] = train['AdjClose'] / train['eps']\n",
    "train['debt_ratio'] = train['total_liabilities'] / train['total_equity']\n",
    "train['debt_to_equity'] = train['total_liabilities'] / train['total_equity']\n",
    "train['roa'] = train['net_income_y'] / train['total_assets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct some additional ticker-level returns features\n",
    "train['open_l1'] = train.groupby('ticker')['Open'].shift(1)\n",
    "train['open_l5'] = train.groupby('ticker')['Open'].shift(5)\n",
    "train['open_l10'] = train.groupby('ticker')['Open'].shift(10)\n",
    "\n",
    "train['return_prev1_open_raw'] = 100*(train['Open'] - train['open_l1'])/train['open_l1']\n",
    "train['return_prev5_open_raw'] = 100*(train['Open'] - train['open_l5'])/train['open_l5']\n",
    "train['return_prev10_open_raw'] = 100*(train['Open'] - train['open_l10'])/train['open_l10']\n",
    "\n",
    "train['close_l1'] = train.groupby('ticker')['AdjClose'].shift(1)\n",
    "train['close_l5'] = train.groupby('ticker')['AdjClose'].shift(5)\n",
    "train['close_l10'] = train.groupby('ticker')['AdjClose'].shift(10)\n",
    "\n",
    "train['return_prev1_close_raw'] = 100*(train['AdjClose'] - train['close_l1'])/train['close_l1']\n",
    "train['return_prev5_close_raw'] = 100*(train['AdjClose'] - train['close_l5'])/train['close_l5']\n",
    "train['return_prev10_close_raw'] = 100*(train['AdjClose'] - train['close_l10'])/train['close_l10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the quarter of pre-SimFin data\n",
    "train = train[train['date_of_transaction'] >= '2011-03-31'].reset_index().drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At the ticker level, lead the AdjClose column by 21 trading days\n",
    "target_gen = train[['ticker', 'date_of_transaction', 'AdjClose']]\n",
    "AdjClose_ahead = target_gen.groupby('ticker')['AdjClose'].shift(-21)\n",
    "AdjClose_ahead.name = 'AdjClose_ahead'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw, month-ahead return is calculated as:\n",
    "$$target_{t,i} = \\frac{AdjClose_{t+21,i} - AdjClose_{t,i}}{AdjClose_{t,i}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct monthly-return target variable (raw returns)\n",
    "target_raw = np.array(100*((AdjClose_ahead - train['AdjClose'])/train['AdjClose']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing all of the returns for the next 21 days (month) relative to today\n",
    "aheads = []\n",
    "for i in range(0,22):\n",
    "    AdjClose_ahead_i = target_gen.groupby('ticker')['AdjClose'].shift(-i)\n",
    "    aheads.append(np.array(100*((AdjClose_ahead_i - train['AdjClose'])/train['AdjClose'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average, raw returns for all periods within the next month is calculated as:\n",
    "$$target_{t,i} = (\\frac{1}{n})\\sum_{k=1}^n \\frac{AdjClose_{t+k,i} - AdjClose_{t,i}}{AdjClose_{t,i}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct various average-returns over the month-ahead range variants\n",
    "target_sma = np.array(pd.DataFrame(aheads).mean(axis=0, skipna=False).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct market-residualized variants (NEED S&P 500 OR CONSTRUCTION OF AN INDEX FROM CURRENT FEATURES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporatory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a feature selection list (THINK ABOUT INFORMING THIS SELECTION WITH SHRINKAGE METHODS, I.E. RIDGE REGRESSION)\n",
    "features = ['High', 'Low', 'Open', 'Close', 'Volume', 'AdjClose', 'Year',\n",
    "            'Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear', 'Pct_Change_Daily',\n",
    "            'Pct_Change_Monthly', 'Pct_Change_Yearly', 'RSI', 'Volatility',\n",
    "            'Yearly_Return_Rank', 'Monthly_Return_Rank', 'Pct_Change_Class',\n",
    "            'Rolling_Yearly_Mean_Positive_Days', 'Rolling_Monthly_Mean_Positive_Days', \n",
    "            'Rolling_Monthly_Mean_Price', 'Rolling_Yearly_Mean_Price',\n",
    "            'open_l1', 'open_l5', 'open_l10', 'close_l1', 'close_l5', 'close_l10',\n",
    "            'return_prev1_open_raw', 'return_prev5_open_raw', 'return_prev10_open_raw',\n",
    "            'return_prev1_close_raw', 'return_prev5_close_raw', 'return_prev10_close_raw',\n",
    "            'pe_ratio', 'debt_ratio', 'debt_to_equity', 'roa'\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select on features to pass to modeling machinery\n",
    "X = train[features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set relevant scikit-learn functions/modules\n",
    "\n",
    "# Tests for stationarity \n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Regression models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Classification models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# LightGBM\n",
    "from lightgbm import LGBMClassifier\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# Model selection\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Evaluation\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Wheat Classification notebook \n",
    "def fit_and_evaluate_classifier(X, y, holdout, ticker, gamma, valid_splits, model, label, param_search={}, export=False, \n",
    "                                **kwargs):\n",
    "    \"\"\"\n",
    "    Because of the Scikit-Learn API, we can create a function to\n",
    "    do all of the fit and evaluate work on our behalf!\n",
    "    \"\"\"\n",
    "    start  = time.time() # Start the clock! \n",
    "    scores = {'precision':[], 'recall':[], 'accuracy':[], 'f1':[]}\n",
    "    importances = []\n",
    "\n",
    "    # Set time-series, cross-validation indices\n",
    "    tscv = TimeSeriesSplit(n_splits=12).split(X)\n",
    "    tscv_grid = TimeSeriesSplit(n_splits=12).split(X)\n",
    "    \n",
    "    # Perform a grid-search on the provided parameters to determine best options\n",
    "    if param_search:\n",
    "        gridsearch = GridSearchCV(estimator=model(), \n",
    "                                  cv=tscv_grid,\n",
    "                                  param_grid=param_search,\n",
    "                                  n_jobs=-1)\n",
    "\n",
    "        # Fit to extract best parameters later\n",
    "        gridsearch_model = gridsearch.fit(X,y)\n",
    "\n",
    "    opt_thresholds = []\n",
    "    for train, test in tscv:\n",
    "        X_train, X_test = X.loc[X.index[train]], X.loc[X.index[test]]\n",
    "        y_train, y_test = y[train], y[test]\n",
    "        expected  = y_test\n",
    "        \n",
    "        # Fit a model given the optimal parameters established in grid search\n",
    "        if param_search:\n",
    "            estimator = model(**gridsearch_model.best_params_)\n",
    "        else:\n",
    "            estimator = model(**kwargs)\n",
    "\n",
    "        # Fit the fitted model on the test set and store positive class probabilities\n",
    "        probs = estimator.predict_proba(X_test)\n",
    "        pos_probs = [p[1] for p in probs]\n",
    "\n",
    "        # Dynamic classification threshold selection\n",
    "        thresholds = list(np.arange(0.30, 0.90, 0.05))\n",
    "        preds = [[1 if y >= t else 0 for y in pos_probs] for t in thresholds]\n",
    "        scores_by_threshold_ = [metrics.f1_score(y_test, p) for p in preds]\n",
    "        opt_thresh_ = thresholds[scores_by_threshold_.index(max(scores_by_threshold_))]\n",
    "        opt_thresholds.append(opt_thresh_)\n",
    "\n",
    "        # Generate class predictions\n",
    "        predicted = [1 if y >= opt_thresh_ else 0 for y in pos_probs]\n",
    "\n",
    "        # Append scores to the tracker\n",
    "        scores['precision'].append(metrics.precision_score(expected, predicted, average=\"weighted\"))\n",
    "        scores['recall'].append(metrics.recall_score(expected, predicted, average=\"weighted\"))\n",
    "        scores['accuracy'].append(metrics.accuracy_score(expected, predicted))\n",
    "        scores['f1'].append(metrics.f1_score(expected, predicted, average=\"weighted\"))\n",
    "        \n",
    "        # Store variable importances \n",
    "        if model in [RandomForestClassifier, GradientBoostingClassifier]:\n",
    "            importances.append(estimator.feature_importances_.tolist())\n",
    "\n",
    "    # Average variable importances across the validation sets\n",
    "    if model in [RandomForestClassifier, GradientBoostingClassifier]:\n",
    "        var_imps = pd.DataFrame(importances).apply(lambda x: np.mean(x)).tolist()\n",
    "        var_imps = list(zip(features, np.round(100*var_imps, decimals=2)))\n",
    "        var_imps = sorted(var_imps, key=lambda x: x[1], reverse=True)\n",
    "    else:\n",
    "        var_imps = []\n",
    "    \n",
    "    # Fit on full sample \n",
    "    if param_search:\n",
    "        estimator = model(**gridsearch_model.best_params_)\n",
    "    else:\n",
    "        estimator = model(**kwargs)\n",
    "        \n",
    "    estimator.fit(X,y)\n",
    "    \n",
    "    # Test on hold out set\n",
    "    out_of_sample_probs = estimator.predict_proba(holdout)\n",
    "    pos_probs = [p[1] for p in out_of_sample_probs]\n",
    "    predicted = [1 if y >= opt_thresh_ else 0 for y in pos_probs]\n",
    "\n",
    "    # Store values for later reporting/use in app\n",
    "    evals = {'precision':np.mean(scores['precision']), \n",
    "             'recall':np.mean(scores['recall']), \n",
    "             'accuracy':np.mean(scores['accuracy']), \n",
    "             'f1':np.mean(scores['f1']),\n",
    "             'probabilities':pos_probs,\n",
    "             'predictions':predicted,\n",
    "             'importances':var_imps\n",
    "             }\n",
    "    \n",
    "    # Report\n",
    "    print(\"Build, hyperparameter selection, and validation of {} took {:0.3f} seconds\\n\".format(label, time.time()-start))\n",
    "    print(\"Hyperparameters are as follows:\")\n",
    "    if param_search:\n",
    "        for key in gridsearch_model.best_params_.keys():\n",
    "            print(\"{}: {}\\n\".format(key, gridsearch_model.best_params_[key]))\n",
    "    print(\"Validation scores are as follows:\")\n",
    "    print(pd.DataFrame(scores).mean())\n",
    "    \n",
    "    # Output the evals dictionary\n",
    "    if export:\n",
    "        outpath = \"{}_{}.pickle\".format(label.tolower().replace(\" \", \"_\"), ticker.tolower())\n",
    "        with open(outpath, 'wb') as f:\n",
    "            pickle.dump(evals, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_lgbm_classifier(X, y, holdout, ticker, gamma=1, valid_splits=12, export=False):\n",
    "    \n",
    "    # Convert to NumPy arrays - store feature names\n",
    "    feature_names = X.columns.tolist()\n",
    "    features = np.array(X)\n",
    "    test_features = np.array(holdout)\n",
    "    labels = y.copy()\n",
    "    \n",
    "    # Compute EMA smoothing of target prior to constructing classes\n",
    "    EMA = 0\n",
    "    gamma_ = gamma\n",
    "    for ti in range(len(labels)):\n",
    "        EMA = gamma_*labels[ti] + (1-gamma_)*EMA\n",
    "        labels[ti] = EMA  \n",
    "        \n",
    "    labels_smoothed = np.where(labels >= 0, 1, 0)\n",
    "\n",
    "    # Generate splits\n",
    "    splits = TimeSeriesSplit(n_splits=valid_splits)\n",
    "\n",
    "    # Empty array for feature importances\n",
    "    feature_importance_values = np.zeros(len(feature_names))\n",
    "\n",
    "    # Empty array for out of fold validation predictions\n",
    "    out_of_fold = np.zeros(features.shape[0])\n",
    "    \n",
    "    # Empty array for test predictions\n",
    "    test_predictions = np.zeros(test_features.shape[0])\n",
    "\n",
    "    # Lists for recording validation and training scores\n",
    "    valid_scores = []\n",
    "    train_scores = []\n",
    "    valid_accs = []\n",
    "    train_accs = []\n",
    "    \n",
    "    # Iterate through each fold\n",
    "    for train_indices, valid_indices in splits.split(X): \n",
    "\n",
    "        # Training data for the fold\n",
    "        train_features, train_labels = features[train_indices], labels[train_indices]\n",
    "        #Validation data for the fold\n",
    "        valid_features, valid_labels = features[valid_indices], labels[valid_indices]\n",
    "\n",
    "        EMA = 0\n",
    "        gamma = 1\n",
    "        for ti in range(len(train_labels)):\n",
    "            EMA = gamma*train_labels[ti] + (1-gamma)*EMA\n",
    "            train_labels[ti] = EMA \n",
    "\n",
    "        train_labels = np.where(train_labels >= 0, 1, 0)\n",
    "        valid_labels = np.where(valid_labels >= 0, 1, 0)\n",
    "\n",
    "        # Create the bst\n",
    "        bst = LGBMClassifier(n_estimators=10000, objective = 'binary', \n",
    "                             class_weight = 'balanced', learning_rate = 0.01,\n",
    "                             max_bin = 25, num_leaves = 50, max_depth = 1,\n",
    "                             reg_alpha = 0.1, reg_lambda = 0.1, \n",
    "                             subsample = 0.8, random_state = 101,\n",
    "                             boosting = 'gbdt'\n",
    "                            )\n",
    "\n",
    "        # Train the bst\n",
    "        bst.fit(train_features, train_labels, eval_metric = ['auc', 'binary_error'],\n",
    "                eval_set = [(valid_features, valid_labels), (train_features, train_labels)],\n",
    "                eval_names = ['valid', 'train'],\n",
    "                early_stopping_rounds = 100, verbose = 0)\n",
    "\n",
    "        # Record the best iteration\n",
    "        best_iteration = bst.best_iteration_\n",
    "\n",
    "        # Record the feature importances\n",
    "        feature_importance_values += bst.feature_importances_ / splits.n_splits\n",
    "        \n",
    "        # Make predictions\n",
    "        test_predictions += bst.predict_proba(test_features, num_iteration = best_iteration)[:, 1] / splits.n_splits\n",
    "\n",
    "        # Record the out of fold predictions\n",
    "        out_of_fold[valid_indices] = bst.predict_proba(valid_features, num_iteration = best_iteration)[:, 1]\n",
    "\n",
    "        # Record the best score\n",
    "        valid_score = bst.best_score_['valid']['auc']\n",
    "        train_score = bst.best_score_['train']['auc']\n",
    "        valid_acc = bst.best_score_['valid']['binary_error']\n",
    "        train_acc = bst.best_score_['train']['binary_error']\n",
    "        \n",
    "        valid_scores.append(valid_score)\n",
    "        train_scores.append(train_score)\n",
    "        valid_accs.append(valid_acc)\n",
    "        train_accs.append(train_acc)\n",
    "        \n",
    "    # Set up an exportable dictionary with results from the model\n",
    "    results = {\n",
    "        'train_auc':train_scores,\n",
    "        'train_acc':train_acc,\n",
    "        'validation_auc':valid_scores,\n",
    "        'validation_accuracy':valid_acc,\n",
    "        'valid_preds':out_of_fold,\n",
    "        'feature_importances':feature_importance_values,\n",
    "        'test_predictions':test_predictions\n",
    "    }\n",
    "    \n",
    "    # Output the results dictionary\n",
    "    if export:\n",
    "        outpath = \"lgbc_{}.pickle\".format(ticker.tolower())\n",
    "        with open(outpath, 'wb') as f:\n",
    "            pickle.dump(results, f)\n",
    "    \n",
    "    print(\"\\nAverage AUC across {} splits: {}\".format(valid_splits, np.mean(valid_scores)))\n",
    "    print(\"Average accuracy across {} splits: {}%\".format(valid_splits, 100*np.round((1-np.mean(valid_acc)),2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Panel-level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that there are bound to be a number of systemic considerations that impact the price of a stock at any given point in time, it is prudent to perform and evaluate predictions across the panel of S&P 500 stocks in our sample, which will capture potential linkages between different stocks, and allow us to explore the possibility of using features generated from clustering to group like stocks in the panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create copies for panel-level regressions\n",
    "X_p = X.copy(deep=True)\n",
    "y_p = y_disc.copy()\n",
    "\n",
    "# Indexes of hold-out test data (the 21 days of data preceding the present day)\n",
    "test_idx = np.where(np.isnan(y_p))[0].tolist()\n",
    "\n",
    "# Simple feature-scaling - for now, replace missings with 0 (i.e. the mean of a normalized feature)\n",
    "X_p = X_p.groupby(['Year', 'Month', 'Day']).apply(lambda x: (x - np.mean(x))/np.std(x)).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove hold-out test data\n",
    "y_p = np.delete(y_p, test_idx)\n",
    "X_p_holdout = X_p.loc[X_p.index[test_idx]]\n",
    "X_p = X_p.drop(X_p.index[test_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and evaluate\n",
    "fit_and_evaluate(X_p, y_p, X_p_holdout,\n",
    "                 KNeighborsClassifier, \"kNN Classifier\", \n",
    "                 param_search={'n_neighbors':[4,5,6]},\n",
    "                 export=False\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ticker-level "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the heart of this analysis is a time-series prediction problem. As such, it is prudent to explore running models for each individual stock. We can envision averaging the results of both modeling approaches to incorporate the contribution of both into a final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of the tickers to loop through\n",
    "tickers = train['ticker'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average AUC across 12 splits: 0.7312507210999152\n",
      "Average accuracy across 12 splits: 72.0%\n",
      "\n",
      "Average AUC across 12 splits: 0.7690786564132078\n",
      "Average accuracy across 12 splits: 65.0%\n",
      "\n",
      "Average AUC across 12 splits: 0.7578165964133973\n",
      "Average accuracy across 12 splits: 78.0%\n",
      "\n",
      "Average AUC across 12 splits: 0.7959639542951753\n",
      "Average accuracy across 12 splits: 92.0%\n",
      "\n",
      "Average AUC across 12 splits: 0.7351982211011419\n",
      "Average accuracy across 12 splits: 79.0%\n"
     ]
    }
   ],
   "source": [
    "for i, t in enumerate(tickers[:5]):\n",
    "    \n",
    "    # Pull only feature/target data for the relevant stocker\n",
    "    X_t = X.loc[train['ticker'] == t,:]\n",
    "    y_t = target_sma[train['ticker'] == t]\n",
    "    \n",
    "    # Indexes of hold-out test data (the 21 days of data preceding the present day)\n",
    "    test_idx = np.where(np.isnan(y_t))[0].tolist()\n",
    "    \n",
    "    # Simple feature-scaling - for now, replace missings with 0 (i.e. the mean of a normalized feature)\n",
    "    X_t = X_t.apply(lambda x: (x - np.mean(x))/np.std(x)).fillna(0)\n",
    "    \n",
    "    # Remove hold-out test data\n",
    "    y_t = np.delete(y_t, test_idx)\n",
    "    X_t_holdout = X_t.loc[X_t.index[test_idx]]\n",
    "    X_t = X_t.drop(X_t.index[test_idx])\n",
    "    \n",
    "    # Fit and evaluate\n",
    "    fit_lgbm_classifier(X_t, y_t, X_t_holdout, gamma=0.05, valid_splits=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1; t = tickers[i] ## for i, t in enumerate(tickers):\n",
    "\n",
    "# Pull only feature/target data for the relevant stocker\n",
    "X_t = X.loc[train['ticker'] == t,:]\n",
    "y_t = target_sma[train['ticker'] == t]\n",
    "\n",
    "# Indexes of hold-out test data (the 21 days of data preceding the present day)\n",
    "test_idx = np.where(np.isnan(y_t))[0].tolist()\n",
    "\n",
    "# Simple feature-scaling - for now, replace missings with 0 (i.e. the mean of a normalized feature)\n",
    "X_t = X_t.apply(lambda x: (x - np.mean(x))/np.std(x)).fillna(0)\n",
    "\n",
    "# Remove hold-out test data\n",
    "y_t = np.delete(y_t, test_idx)\n",
    "X_t_holdout = X_t.loc[X_t.index[test_idx]]\n",
    "X_t = X_t.drop(X_t.index[test_idx])\n",
    "\n",
    "# Fit and evaluate\n",
    "fit_lgbm_classifier(X_t, y_t, X_t_holdout, gamma=0.1, valid_splits=12)\n",
    "#fit_and_evaluate(X_t, y_t, X_t_holdout,\n",
    "#                 KNeighborsClassifier, \"kNN Classifier\", \n",
    "#                 param_search={'n_neighbors':[2,4,6,8,10,12]},\n",
    "#                 export=False\n",
    "#                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
