{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Price Classification\n",
    "By: Jared Berry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('always') # To deal with 'UndefinedMetric' warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\__init__.py:30: ResourceWarning: unclosed file <_io.TextIOWrapper name='C:\\\\Users\\\\jared\\\\Anaconda3\\\\lib\\\\site-packages\\\\lightgbm\\\\VERSION.txt' mode='r' encoding='cp1252'>\n",
      "  __version__ = open(os.path.join(dir_path, 'VERSION.txt')).read().strip()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "# I/O and data structures\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Classification models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# Model selection\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Evaluation\n",
    "from sklearn import metrics\n",
    "\n",
    "# Quality of life\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modeling helper functions\n",
    "from modeling_funcs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "inpath = \"model_dictionary.pickle\"\n",
    "with open(inpath, 'rb') as f:\n",
    "    modeling = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['target_1_return', 'target_1_return_res', 'target_1_composite', 'target_1_average', 'target_1_rank', 'target_1_up', 'target_1_rel_return', 'target_1_rel_up', 'target_5_return', 'target_5_return_res', 'target_5_composite', 'target_5_average', 'target_5_rank', 'target_5_up', 'target_5_rel_return', 'target_5_rel_up', 'target_10_return', 'target_10_return_res', 'target_10_composite', 'target_10_average', 'target_10_rank', 'target_10_up', 'target_10_rel_return', 'target_10_rel_up', 'target_21_return', 'target_21_return_res', 'target_21_composite', 'target_21_average', 'target_21_rank', 'target_21_up', 'target_21_rel_return', 'target_21_rel_up', 'features', 'ticker_features'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modeling.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull out the features dataframe\n",
    "train = modeling['features']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a feature selection list (THINK ABOUT INFORMING THIS SELECTION WITH SHRINKAGE METHODS, I.E. RIDGE REGRESSION)\n",
    "features = ['High', 'Low', 'Open', 'Close', 'Volume', 'AdjClose', 'Year',\n",
    "            'Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear', 'Pct_Change_Daily',\n",
    "            'Pct_Change_Monthly', 'Pct_Change_Yearly', 'RSI', 'Volatility',\n",
    "            'Yearly_Return_Rank', 'Monthly_Return_Rank', 'Pct_Change_Class',\n",
    "            'Rolling_Yearly_Mean_Positive_Days', 'Rolling_Monthly_Mean_Positive_Days', \n",
    "            'Rolling_Monthly_Mean_Price', 'Rolling_Yearly_Mean_Price',\n",
    "            'Momentum_Quality_Monthly', 'Momentum_Quality_Yearly', 'SPY_Trailing_Month_Return',\n",
    "            'open_l1', 'open_l5', 'open_l10', 'close_l1', 'close_l5', 'close_l10',\n",
    "            'return_prev1_open_raw', 'return_prev5_open_raw', 'return_prev10_open_raw',\n",
    "            'return_prev1_close_raw', 'return_prev5_close_raw', 'return_prev10_close_raw',\n",
    "            'pe_ratio', 'debt_ratio', 'debt_to_equity', 'roa',\n",
    "            'beta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select on features to pass to modeling machinery, along with necessary indexers\n",
    "X = train[features]\n",
    "tickers = train['ticker'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a ticker\n",
    "target = modeling['target_21_rel_up']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Panel-level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that there are bound to be a number of systemic considerations that impact the price of a stock at any given point in time, it is prudent to perform and evaluate predictions across the panel of S&P 500 stocks in our sample, which will capture potential linkages between different stocks, and allow us to explore the possibility of using features generated from clustering to group like stocks in the panel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a panel-level copy\n",
    "y_p = target.copy()\n",
    "\n",
    "# Indexes of hold-out test data (the 21 days of data preceding the present day)\n",
    "test_idx = np.where(np.isnan(y_p))[0].tolist()\n",
    "\n",
    "# In order to ensure grouping is done properly, remove this data from a ticker-identification set as well\n",
    "ticker_locs = train['ticker'].drop(train.index[test_idx]).reset_index().drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a panel-level copy\n",
    "X_p = X.copy(deep=True)\n",
    "\n",
    "# Simple feature-scaling - for now, replace missings with 0 (i.e. the mean of a normalized feature) within days\n",
    "X_p = X_p.groupby(['Year', 'Month', 'Day']).apply(lambda x: (x - np.mean(x))/np.std(x)).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove hold-out test data\n",
    "y_p = np.delete(y_p, test_idx)\n",
    "X_p_holdout = X_p.loc[X_p.index[test_idx]]\n",
    "X_p = X_p.drop(X_p.index[test_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_p_smoothed = np.zeros(y_p.shape[0])\n",
    "for t in tickers:\n",
    "    idx = ticker_locs.loc[ticker_locs['ticker'] == t].index.tolist()\n",
    "    y_to_smooth = y_p[idx]\n",
    "    \n",
    "    # Compute EMA smoothing of target within ticker\n",
    "    EMA = 0\n",
    "    gamma_ = 1\n",
    "    for ti in range(len(y_to_smooth)):\n",
    "        EMA = gamma_*y_to_smooth[ti] + (1-gamma_)*EMA\n",
    "        y_to_smooth[ti] = EMA\n",
    "        \n",
    "    y_p_smoothed[idx] = y_to_smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_p_smoothed = y_p.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and evaluate - gamma MUST be 1 here\n",
    "fit_lgbm_classifier(X_p, y_p_smoothed, X_p_holdout, ticker=\"\", ema_gamma=1, n_splits=12, export=False, \n",
    "                    cv_method = 'ts', groups = ticker_locs, labeled = False)\n",
    "#fit_sklearn_classifier(X_p, y_p_smoothed, X_p_holdout, ticker=\"\", ema_gamma=1, n_splits=12, model=KNeighborsClassifier,\n",
    "#                       label='kNN Classifier', param_search = {}, export=False, labeled=False, groups=ticker_locs,\n",
    "#                       cv_method='panel'\n",
    "#                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ticker-level "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the heart of this analysis is a time-series prediction problem. As such, it is prudent to explore running models for each individual stock. We can envision averaging the results of both modeling approaches to incorporate the contribution of both into a final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0 targets changed by smoothing.\n",
      "Performing grid search for hyperparameter tuning\n"
     ]
    }
   ],
   "source": [
    "for i, t in enumerate(tickers[:5]):\n",
    "    \n",
    "    # Pull only feature/target data for the relevant stocker\n",
    "    X_t = X.loc[train['ticker'] == t,:]\n",
    "    y_t = np.array(target)[train['ticker'] == t]\n",
    "    \n",
    "    # Indexes of hold-out test data (the 21 days of data preceding the present day)\n",
    "    test_idx = np.where(np.isnan(y_t))[0].tolist()\n",
    "    \n",
    "    # Simple feature-scaling - for now, replace missings with 0 (i.e. the mean of a normalized feature)\n",
    "    X_t = X_t.apply(lambda x: (x - np.mean(x))/np.std(x)).fillna(0)\n",
    "    \n",
    "    # Remove hold-out test data\n",
    "    y_t = np.delete(y_t, test_idx)\n",
    "    X_t_holdout = X_t.loc[X_t.index[test_idx]]\n",
    "    X_t = X_t.drop(X_t.index[test_idx])\n",
    "    \n",
    "    # Fit and evaluate\n",
    "    model_dict = \\\n",
    "    fit_lgbm_classifier(X_t, \n",
    "                        y_t,\n",
    "                        X_t_holdout, \n",
    "                        ticker=t, \n",
    "                        ema_gamma=1, \n",
    "                        valid_splits=12,\n",
    "                        cv_method='ts', \n",
    "                        labeled = False,\n",
    "                        param_search = {'learning_rate':[0.1, 0.01],\n",
    "                                        'n_estimators':[1000,10000],\n",
    "                                        'max_bin':[25,50,75,100],\n",
    "                                        'num_leaves':[25,50,75,100]\n",
    "                                       }\n",
    "                       )\n",
    "    #fit_sklearn_classifier(X_t, y_t, X_t_holdout, ticker=t, ema_gamma=0.1, n_splits=12, model=KNeighborsClassifier,\n",
    "    #                       label='kNN Classifier', param_search = {'n_neighbors':[2,4,6,8,10]}, \n",
    "    #                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, auc, roc_curve, f1_score\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "def basic_random_forest_classifier(stock_features, stock_labels, n_splits=2):\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(stock_features, stock_labels, test_size=test_size)\n",
    "    tseries = TimeSeriesSplit(n_splits)\n",
    "    test_results = []\n",
    "    for train_index, test_index in tseries.split(stock_features):  \n",
    "        X_train, X_test = stock_features.iloc[train_index], stock_features.iloc[test_index]\n",
    "        y_train, y_test = stock_labels[train_index], stock_labels[test_index]\n",
    "        model = RandomForestClassifier(n_estimators=16)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        expected = y_test\n",
    "        predicted = model.predict(X_test)\n",
    "        \n",
    "        false_positive_rate, true_positive_rate, thresholds = roc_curve(expected, predicted)\n",
    "        roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "        \"\"\"\n",
    "        print(roc_auc)\n",
    "        print(false_positive_rate)\n",
    "        print(true_positive_rate)\n",
    "        print(thresholds)\n",
    "        \"\"\"\n",
    "\n",
    "        print('Random Forest model')\n",
    "        #print(\"Stock Percent Up Days: \", expected.mean(), \"\\n\\n\")\n",
    "        print(\"F1:\",f1_score(expected, predicted))\n",
    "        print(\"accuracy: \", accuracy_score(expected, predicted))\n",
    "        \n",
    "        print(classification_report(expected, predicted))\n",
    "        print(confusion_matrix(expected, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_random_forest_classifier(X_p, np.where(y_p_smoothed > 0, 1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
