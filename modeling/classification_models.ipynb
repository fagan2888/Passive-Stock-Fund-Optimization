{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Price Classification\n",
    "By: Jared Berry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quality of life\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "\n",
    "# I/O and data structures\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Classification models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Model selection\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Evaluation\n",
    "from sklearn import metrics\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic\n",
    "%matplotlib inline\n",
    "%load_ext pycodestyle_magic\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modeling helper functions\n",
    "from modeling_funcs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "inpath = \"model_dictionary.pickle\"\n",
    "with open(inpath, 'rb') as f:\n",
    "    modeling = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull out the features dataframe\n",
    "train = modeling['features']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a feature selection list (THINK ABOUT INFORMING THIS SELECTION WITH SHRINKAGE METHODS, I.E. LASSO REGRESSION)\n",
    "features = ['High', 'Low', 'Open', 'Close', 'Volume', 'AdjClose', 'Year',\n",
    "            'Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear', 'Pct_Change_Daily',\n",
    "            'Pct_Change_Monthly', 'Pct_Change_Yearly', 'RSI', 'Volatility',\n",
    "            'Yearly_Return_Rank', 'Monthly_Return_Rank', 'Pct_Change_Class',\n",
    "            'Rolling_Yearly_Mean_Positive_Days', 'Rolling_Monthly_Mean_Positive_Days', \n",
    "            'Rolling_Monthly_Mean_Price', 'Rolling_Yearly_Mean_Price',\n",
    "            'Momentum_Quality_Monthly', 'Momentum_Quality_Yearly', 'SPY_Trailing_Month_Return',\n",
    "            'open_l1', 'open_l5', 'open_l10', 'close_l1', 'close_l5', 'close_l10',\n",
    "            'return_prev1_open_raw', 'return_prev5_open_raw', 'return_prev10_open_raw',\n",
    "            'return_prev1_close_raw', 'return_prev5_close_raw', 'return_prev10_close_raw',\n",
    "            'pe_ratio', 'debt_ratio', 'debt_to_equity', 'roa',\n",
    "            'beta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select on features to pass to modeling machinery, along with necessary indexers\n",
    "X = train[features]\n",
    "tickers = train['ticker'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a ticker\n",
    "target = modeling['target_21_rel_return']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Panel-level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that there are bound to be a number of systemic considerations that impact the price of a stock at any given point in time, it is prudent to perform and evaluate predictions across the panel of S&P 500 stocks in our sample, which will capture potential linkages between different stocks, and allow us to explore the possibility of using features generated from clustering to group like stocks in the panel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a panel-level copy\n",
    "y_p = target.copy()\n",
    "\n",
    "# Indexes of hold-out test data (the 21 days of data preceding the present day)\n",
    "test_idx = np.where(np.isnan(y_p))[0].tolist()\n",
    "\n",
    "# In order to ensure grouping is done properly, remove this data from a ticker-identification set as well\n",
    "ticker_locs = train[['ticker','date_of_transaction']].drop(train.index[test_idx]).reset_index().drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a panel-level copy; normalize by day\n",
    "X_p = X.copy(deep=True)\n",
    "X_p = X_p.groupby(['Year', 'Month', 'Day']).apply(lambda x: (x - np.mean(x))/np.std(x)).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove hold-out test data\n",
    "y_p = np.delete(y_p, test_idx)\n",
    "X_p_holdout = X_p.loc[X_p.index[test_idx]]\n",
    "X_p = X_p.drop(X_p.index[test_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exponential Moving Average smoothing (skip if not)\n",
    "y_p_smoothed = np.zeros(y_p.shape[0])\n",
    "for t in tickers:\n",
    "    idx = ticker_locs.loc[ticker_locs['ticker'] == t].index.tolist()\n",
    "    y_to_smooth = y_p[idx]\n",
    "    \n",
    "    # Compute EMA smoothing of target within ticker\n",
    "    EMA = 0\n",
    "    gamma_ = 1\n",
    "    for ti in range(len(y_to_smooth)):\n",
    "        EMA = gamma_*y_to_smooth[ti] + (1-gamma_)*EMA\n",
    "        y_to_smooth[ti] = EMA\n",
    "        \n",
    "    y_p_smoothed[idx] = y_to_smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_p_smoothed = y_p.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LGBM\n",
    "model_dict = fit_lgbm_classifier(X_p, \n",
    "                                 y_p_smoothed, \n",
    "                                 X_p_holdout, \n",
    "                                 ticker=\"\", \n",
    "                                 ema_gamma=1, \n",
    "                                 n_splits=12,\n",
    "                                 cv_method='ts',\n",
    "                                 groups=ticker_locs, \n",
    "                                 labeled=False,\n",
    "                                 label=\"LGBM Classifier\",\n",
    "                                 param_search=None,\n",
    "                                 holdout_method='distributed',\n",
    "                                 threshold_search=False,\n",
    "                                 export=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression\n",
    "model_dict = fit_sklearn_classifier(X_p, \n",
    "                                    y_p, \n",
    "                                    X_p_holdout, \n",
    "                                    ticker=\"\", \n",
    "                                    ema_gamma=1, \n",
    "                                    n_splits=12,\n",
    "                                    cv_method='ts',\n",
    "                                    model=LogisticRegression,\n",
    "                                    label='Logit', \n",
    "                                    param_search=None,\n",
    "                                    holdout_method='distributed',\n",
    "                                    threshold_search=False,\n",
    "                                    n_jobs=-1,\n",
    "                                    export=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kNN\n",
    "model_dict = fit_sklearn_classifier(X_p, \n",
    "                                    y_p, \n",
    "                                    X_p_holdout, \n",
    "                                    ticker=\"\", \n",
    "                                    ema_gamma=1, \n",
    "                                    n_splits=12,\n",
    "                                    cv_method='ts',\n",
    "                                    model=KNeighborsClassifier,\n",
    "                                    label='kNN Classifier', \n",
    "                                    param_search=None,\n",
    "                                    holdout_method='distributed',\n",
    "                                    threshold_search=False,\n",
    "                                    n_jobs=-1,\n",
    "                                    export=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForest\n",
    "model_dict = fit_sklearn_classifier(X_p, \n",
    "                                    y_p, \n",
    "                                    X_p_holdout, \n",
    "                                    ticker=\"\", \n",
    "                                    ema_gamma=1, \n",
    "                                    n_splits=12,\n",
    "                                    cv_method='ts',\n",
    "                                    model=RandomForestClassifier,\n",
    "                                    label='RandomForest Classifier', \n",
    "                                    param_search=None,\n",
    "                                    holdout_method='distributed',\n",
    "                                    threshold_search=False,\n",
    "                                    n_jobs=-1,\n",
    "                                    export=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GBC\n",
    "model_dict = fit_sklearn_classifier(X_p, \n",
    "                                    y_p, \n",
    "                                    X_p_holdout, \n",
    "                                    ticker=\"\", \n",
    "                                    ema_gamma=1, \n",
    "                                    n_splits=12,\n",
    "                                    cv_method='ts',\n",
    "                                    model=GradientBoostingClassifier,\n",
    "                                    label='Boosting Classifier', \n",
    "                                    param_search=None,\n",
    "                                    holdout_method='distributed',\n",
    "                                    threshold_search=False,\n",
    "                                    n_jobs=-1,\n",
    "                                    export=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = model_dict['preds_df']\n",
    "test = test[test['split_number'] != 0]\n",
    "print(metrics.confusion_matrix(test['expected'], test['predicted']))\n",
    "print(metrics.roc_auc_score(test['expected'], test['predicted']))\n",
    "print(metrics.classification_report(test['expected'], test['predicted']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ticker-level "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the heart of this analysis is a time-series prediction problem. As such, it is prudent to explore running models for each individual stock. We can envision averaging the results of both modeling approaches to incorporate the contribution of both into a final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dfs = []\n",
    "for i, t in enumerate(tickers[:50]):\n",
    "    \n",
    "    # Pull only feature/target data for the relevant stocker\n",
    "    X_t = X.loc[train['ticker'] == t,:]\n",
    "    y_t = np.array(target)[train['ticker'] == t]\n",
    "    \n",
    "    # Indexes of hold-out test data (the 21 days of data preceding the present day)\n",
    "    test_idx = np.where(np.isnan(y_t))[0].tolist()\n",
    "    \n",
    "    # Simple feature-scaling - for now, replace missings with 0 (i.e. the mean of a normalized feature)\n",
    "    X_t = X_t.apply(lambda x: (x - np.mean(x))/np.std(x)).fillna(0)\n",
    "    \n",
    "    # Remove hold-out test data\n",
    "    y_t = np.delete(y_t, test_idx)\n",
    "    X_t_holdout = X_t.loc[X_t.index[test_idx]]\n",
    "    X_t = X_t.drop(X_t.index[test_idx])\n",
    "    \n",
    "    # Fit and evaluate\n",
    "    model_dict = fit_lgbm_classifier(X_t, \n",
    "                                     y_t,\n",
    "                                     X_t_holdout, \n",
    "                                     ticker=t, \n",
    "                                     ema_gamma=1, \n",
    "                                     n_splits=12,\n",
    "                                     cv_method='tswindow', \n",
    "                                     labeled = False,\n",
    "                                     param_search = {},\n",
    "                                     holdout_method='full',\n",
    "                                     threshold_search = True)\n",
    "\n",
    "    results_dfs.append(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dfs = []\n",
    "for i, t in enumerate(tickers[:1]):\n",
    "    \n",
    "    # Pull only feature/target data for the relevant stocker\n",
    "    X_t = X.loc[train['ticker'] == t,:]\n",
    "    y_t = np.array(target)[train['ticker'] == t]\n",
    "    \n",
    "    # Indexes of hold-out test data (the 21 days of data preceding the present day)\n",
    "    test_idx = np.where(np.isnan(y_t))[0].tolist()\n",
    "    \n",
    "    # Simple feature-scaling - for now, replace missings with 0 (i.e. the mean of a normalized feature)\n",
    "    X_t = X_t.apply(lambda x: (x - np.mean(x))/np.std(x)).fillna(0)\n",
    "    \n",
    "    # Remove hold-out test data\n",
    "    y_t = np.delete(y_t, test_idx)\n",
    "    X_t_holdout = X_t.loc[X_t.index[test_idx]]\n",
    "    X_t = X_t.drop(X_t.index[test_idx])\n",
    "    \n",
    "    # Fit and evaluate\n",
    "    model_dict = fit_sklearn_classifier(X_t, \n",
    "                                        y_t, \n",
    "                                        X_t_holdout, \n",
    "                                        ticker=t, \n",
    "                                        ema_gamma=1, \n",
    "                                        n_splits=12,\n",
    "                                        cv_method='tswindow',\n",
    "                                        model=RandomForestClassifier,\n",
    "                                        label='RF Classifier', \n",
    "                                        param_search = {},\n",
    "                                        holdout_method='full',\n",
    "                                        threshold_search = True,\n",
    "                                        n_estimators = 100,\n",
    "                                        export=True)\n",
    "    \n",
    "    results_dfs.append(model_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Panel-level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ticker-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stand up results dataframes\n",
    "performance_dfs = []\n",
    "feature_importance_dfs = []\n",
    "holdout_predictions = defaultdict(list)\n",
    "\n",
    "for r in results_dfs:\n",
    "    performance_dfs.append(r['preds_df'])\n",
    "    feature_importance_dfs.append(pd.DataFrame(r['feature_importances'], columns=['feature', 'importance']))\n",
    "    holdout_predictions[r['preds_df'].ticker.unique().tolist()[0]] = r['holdout_probs']\n",
    "    \n",
    "ticker_performance = pd.concat(performance_dfs, axis=0)\n",
    "feature_importances = pd.concat(feature_importance_dfs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unpopulated splits (training data never used for validation)\n",
    "ticker_performance = ticker_performance[ticker_performance['split_number'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average feature importances across all ticker-level models\n",
    "average_importances = feature_importances.groupby('feature').mean().sort_values('importance')\n",
    "\n",
    "average_importances.plot(kind='barh', title=\"Feature Importances\", legend=False, figsize=(12,12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC Curve\n",
    "fpr, tpr, threshold = metrics.roc_curve(ticker_performance['expected'], ticker_performance['predicted'])\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision-Recall Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
