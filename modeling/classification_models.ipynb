{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Price Classification\n",
    "By: Jared Berry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set relevant scikit-learn functions/modules\n",
    "\n",
    "# Tests for stationarity \n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Regression models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Classification models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# LightGBM\n",
    "from lightgbm import LGBMClassifier\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# Model selection\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Evaluation\n",
    "from sklearn import metrics\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model_structures(X, y, holdout, labeled=False, ema_gamma=1):\n",
    "    \"\"\"\n",
    "    Given a dataframe of features, a target array, and\n",
    "    holdout test set; label and smooth if necessary.\n",
    "    Returns a tuple of prepared structures for modeling\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert to NumPy arrays - store feature names\n",
    "    feature_names = X.columns.tolist()\n",
    "    features = np.array(X)\n",
    "    test_features = np.array(holdout)\n",
    "    targets = y.copy()\n",
    "    \n",
    "    if labeled:\n",
    "        targets_smoothed = targets.copy()\n",
    "        orig_targets = targets.copy()\n",
    "    else:\n",
    "        # Compute EMA smoothing of target prior to constructing classes\n",
    "        EMA = 0\n",
    "        gamma_ = ema_gamma\n",
    "        for ti in range(len(targets)):\n",
    "            EMA = gamma_*targets[ti] + (1-gamma_)*EMA\n",
    "            targets[ti] = EMA  \n",
    "\n",
    "        targets_smoothed = np.where(targets > 0, 1, 0)    \n",
    "        orig_targets = np.where(y.copy() > 0, 1, 0)\n",
    "        print(\"\\n{} targets changed by smoothing.\".format(np.sum(targets_smoothed != orig_targets)))\n",
    "        \n",
    "    return features, feature_names, test_features, targets_smoothed, orig_targets\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_target(target, groups):\n",
    "    \"\"\"\n",
    "    Benchmark classification metrics for the target\n",
    "    against a one-class target and random-walk per\n",
    "    time-series literature; allows a grouping object\n",
    "    to conduct the random-walk shifting at the entity\n",
    "    level.\n",
    "    Returns nothing; prints benchmark statistics.\n",
    "    \"\"\"\n",
    "    one_class = np.ones(len(target), dtype='int')\n",
    "    if groups:\n",
    "        pass\n",
    "    else:\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Wheat Classification notebook \n",
    "def fit_sklearn_classifier(X, y, holdout, ticker, ema_gamma, valid_splits, model, label, param_search={}, export=False,\n",
    "                           valid_method=\"ts\", labeled=False, groups=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Flexible function for fitting any number of sci-kit learn\n",
    "    classifiers, with optional grid search.\n",
    "    \"\"\"\n",
    "    \n",
    "    warnings.filterwarnings('always')\n",
    "    start = time.time()\n",
    "    \n",
    "    # Prepare modeling structures\n",
    "    features, feature_names, test_features, targets_smoothed, orig_targets = \\\n",
    "    prepare_model_structures(X, y, holdout, labeled, ema_gamma)\n",
    "\n",
    "    # Compute some baselines\n",
    "    all_pos_acc = np.mean(np.ones(test_features.shape[0], dtype='int') == labels_smoothed)\n",
    "    random_walk_classes = np.array(pd.Series(labels_smoothed).shift(1).tolist()[1:], dtype=\"int\") # Inappropriate for panel\n",
    "    rw_acc = np.mean(random_walk_classes == np.array(labels_smoothed[1:], dtype=\"int\"))\n",
    "    print(\"Baseline accuracy is: {}%\".format(100*np.round(all_pos_acc, 2)))\n",
    "    print(\"RW accuracy is {}%\".format(100*np.round(rw_acc, 2)))\n",
    "    \n",
    "    # Set time-series, cross-validation indices\n",
    "    if valid_method == \"panel\":\n",
    "        splits = PanelSplit(n_folds=valid_splits, groups=groups)\n",
    "        search_splits = PanelSplit(n_folds=valid_splits, groups=groups)\n",
    "    elif valid_method == \"ts\":\n",
    "        splits = TimeSeriesSplit(n_splits=valid_splits).split(X)\n",
    "        search_splits = TimeSeriesSplit(n_splits=valid_splits).split(X)\n",
    "    elif valid_method == \"kfold\":\n",
    "        splits = KFold(n_splits = valid_splits).split(X)\n",
    "        search_splits = Kfold(n_splits = valid_splits).split(X)\n",
    "    \n",
    "    # Empty array for feature importances\n",
    "    feature_importance_values = np.zeros(len(feature_names))\n",
    "\n",
    "    # Empty array for out of fold validation predictions\n",
    "    out_of_fold = np.zeros(features.shape[0])\n",
    "    \n",
    "    # Empty array for test predictions\n",
    "    test_predictions = np.zeros(test_features.shape[0])\n",
    "    \n",
    "    # Dictionary of lists for recording validation and training scores\n",
    "    scores = {'precision':[], 'recall':[], 'accuracy':[], 'f1':[]}\n",
    "    \n",
    "    # Perform a grid-search on the provided parameters to determine best options\n",
    "    if param_search:\n",
    "        print(\"Performing grid search for hyperparameter tuning\")\n",
    "        gridsearch = GridSearchCV(estimator=model(), \n",
    "                                  cv=search_splits,\n",
    "                                  param_grid=param_search)\n",
    "\n",
    "        # Fit to extract best parameters later\n",
    "        gridsearch_model = gridsearch.fit(features, labels_smoothed)\n",
    "\n",
    "    opt_thresholds = []; split_counter = 1\n",
    "    for train_indices, valid_indices in splits:\n",
    "        print(\"Training model on validation split #{}\".format(split_counter))\n",
    "        # Training data for the fold\n",
    "        train_features, train_labels = features[train_indices], labels_smoothed[train_indices]\n",
    "        #Validation data for the fold\n",
    "        valid_features, valid_labels = features[valid_indices], labels_smoothed[valid_indices]\n",
    "        \n",
    "        expected = valid_labels\n",
    "        \n",
    "        # Generate a model given the optimal parameters established in grid search\n",
    "        if param_search:\n",
    "            estimator = model(**gridsearch_model.best_params_)\n",
    "        else:\n",
    "            estimator = model(**kwargs)\n",
    "            \n",
    "        # Train the estimator\n",
    "        estimator.fit(train_features, train_labels)\n",
    "\n",
    "        # Fit the fitted model on the test set and store positive class probabilities\n",
    "        probs = estimator.predict_proba(valid_features)\n",
    "        pos_probs = [p[1] for p in probs]\n",
    "\n",
    "        # Dynamic classification threshold selection\n",
    "        thresholds = list(np.arange(0.30, 0.90, 0.05))\n",
    "        preds = [[1 if y >= t else 0 for y in pos_probs] for t in thresholds]\n",
    "        scores_by_threshold_ = [metrics.f1_score(valid_labels, p) for p in preds]\n",
    "        opt_thresh_ = thresholds[scores_by_threshold_.index(max(scores_by_threshold_))]\n",
    "        opt_thresholds.append(opt_thresh_)\n",
    "\n",
    "        # Generate class predictions\n",
    "        predicted = [1 if y >= opt_thresh_ else 0 for y in pos_probs]\n",
    "\n",
    "        # Append scores to the tracker\n",
    "        scores['precision'].append(metrics.precision_score(expected, predicted, average=\"weighted\"))\n",
    "        scores['recall'].append(metrics.recall_score(expected, predicted, average=\"weighted\"))\n",
    "        scores['accuracy'].append(metrics.accuracy_score(expected, predicted))\n",
    "        scores['f1'].append(metrics.f1_score(expected, predicted, average=\"weighted\"))\n",
    "        \n",
    "        # Store variable importances \n",
    "        if model in [RandomForestClassifier, GradientBoostingClassifier]:\n",
    "            feature_importance_values += estimator.feature_importances_ / valid_splits\n",
    "            \n",
    "        # Iterate counter\n",
    "        split_counter += 1\n",
    "            \n",
    "    # Properly format feature importances\n",
    "    named_importances = list(zip(feature_names, feature_importance_values))\n",
    "    sorted_importances = sorted(named_importances, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Fit on full sample \n",
    "    if param_search:\n",
    "        estimator = model(**gridsearch_model.best_params_)\n",
    "    else:\n",
    "        estimator = model(**kwargs)\n",
    "        \n",
    "    estimator.fit(features, labels_smoothed)\n",
    "    \n",
    "    # Test on hold out set\n",
    "    out_of_sample_probs = estimator.predict_proba(holdout)\n",
    "    pos_probs = [p[1] for p in out_of_sample_probs]\n",
    "    predicted = [1 if y >= opt_thresh_ else 0 for y in pos_probs]\n",
    "\n",
    "    # Store values for later reporting/use in app\n",
    "    evals = {'precision':np.mean(scores['precision']), \n",
    "             'recall':np.mean(scores['recall']), \n",
    "             'accuracy':np.mean(scores['accuracy']), \n",
    "             'f1':np.mean(scores['f1']),\n",
    "             'probabilities':pos_probs,\n",
    "             'predictions':predicted,\n",
    "             'importances':sorted_importances\n",
    "             }\n",
    "    \n",
    "    # Report\n",
    "    print(\"Build, hyperparameter selection, and validation of {} took {:0.3f} seconds\\n\".format(label, time.time()-start))\n",
    "    print(\"Hyperparameters are as follows:\")\n",
    "    if param_search:\n",
    "        for key in gridsearch_model.best_params_.keys():\n",
    "            print(\"{}: {}\\n\".format(key, gridsearch_model.best_params_[key]))\n",
    "    print(\"Validation scores are as follows:\")\n",
    "    print(pd.DataFrame(scores).mean())\n",
    "    \n",
    "    # Output the evals dictionary\n",
    "    if export:\n",
    "        outpath = \"{}_{}.pickle\".format(label.tolower().replace(\" \", \"_\"), ticker.tolower())\n",
    "        with open(outpath, 'wb') as f:\n",
    "            pickle.dump(evals, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_lgbm_classifier(X, y, holdout, ticker, ema_gamma=1, valid_splits=12, export=False, valid_method=\"ts\", groups=None,\n",
    "                       labeled=False):\n",
    "    \n",
    "    # Convert to NumPy arrays - store feature names\n",
    "    feature_names = X.columns.tolist()\n",
    "    features = np.array(X)\n",
    "    test_features = np.array(holdout)\n",
    "    labels = y.copy()\n",
    "    \n",
    "    # Compute EMA smoothing of target prior to constructing classes\n",
    "    EMA = 0\n",
    "    gamma_ = ema_gamma\n",
    "    for ti in range(len(labels)):\n",
    "        EMA = gamma_*labels[ti] + (1-gamma_)*EMA\n",
    "        labels[ti] = EMA\n",
    "        \n",
    "    if labeled:\n",
    "        labels_smoothed = labels.copy()\n",
    "        orig_lables = labels.copy()\n",
    "    else:\n",
    "        # Compute EMA smoothing of target prior to constructing classes\n",
    "        EMA = 0\n",
    "        gamma_ = ema_gamma\n",
    "        for ti in range(len(labels)):\n",
    "            EMA = gamma_*labels[ti] + (1-gamma_)*EMA\n",
    "            labels[ti] = EMA  \n",
    "\n",
    "        labels_smoothed = np.where(labels > 0, 1, 0)    \n",
    "        orig_labels = np.where(y.copy() > 0, 1, 0)\n",
    "        print(\"\\n{} labels changed by smoothing.\".format(np.sum(labels_smoothed != orig_labels)))\n",
    "\n",
    "    # Compute some baselines\n",
    "    all_pos_acc = np.mean(np.ones(test_features.shape[0], dtype='int') == labels_smoothed)\n",
    "    random_walk_classes = np.array(pd.Series(labels_smoothed).shift(1).tolist()[1:], dtype=\"int\")\n",
    "    rw_acc = np.mean(random_walk_classes == np.array(labels_smoothed[1:], dtype=\"int\"))\n",
    "    print(\"Baseline accuracy is: {}%\".format(100*np.round(all_pos_acc, 2)))\n",
    "    print(\"RW accuracy is {}%\".format(100*np.round(rw_acc, 2)))\n",
    "    \n",
    "    ## pd.DataFrame(list(zip(labels_smoothed, y.copy()))).plot()\n",
    "\n",
    "    # Set time-series, cross-validation indices\n",
    "    if valid_method == \"panel\":\n",
    "        splits = PanelSplit(n_folds=valid_splits, groups=groups)\n",
    "    elif valid_method == \"ts\":\n",
    "        splits = TimeSeriesSplit(n_splits=valid_splits).split(X)\n",
    "    elif valid_method == \"kfold\":\n",
    "        splits = KFold(n_splits = valid_splits).split(X)\n",
    "\n",
    "    # Empty array for feature importances\n",
    "    feature_importance_values = np.zeros(len(feature_names))\n",
    "\n",
    "    # Empty array for out of fold validation predictions\n",
    "    out_of_fold = np.zeros(features.shape[0])\n",
    "    \n",
    "    # Empty array for test predictions\n",
    "    test_predictions = np.zeros(test_features.shape[0])\n",
    "\n",
    "    # Lists for recording validation and training scores\n",
    "    valid_scores = []\n",
    "    train_scores = []\n",
    "    valid_accs = []\n",
    "    train_accs = []\n",
    "    \n",
    "    # Iterate through each fold\n",
    "    for train_indices, valid_indices in splits: \n",
    "\n",
    "        # Training data for the fold\n",
    "        train_features, train_labels = features[train_indices], labels_smoothed[train_indices]\n",
    "        #Validation data for the fold\n",
    "        valid_features, valid_labels = features[valid_indices], labels_smoothed[valid_indices]\n",
    "\n",
    "        # Create the bst\n",
    "        bst = LGBMClassifier(n_estimators=10000, objective = 'binary', \n",
    "                             class_weight = 'balanced', learning_rate = 0.01,\n",
    "                             max_bin = 50, num_leaves = 50, max_depth = 2,\n",
    "                             reg_alpha = 0.1, reg_lambda = 0.1, \n",
    "                             subsample = 0.8, random_state = 101,\n",
    "                             boosting = 'gbdt'\n",
    "                            )\n",
    "\n",
    "        # Train the bst\n",
    "        bst.fit(train_features, train_labels, eval_metric = ['auc', 'binary_error'],\n",
    "                eval_set = [(valid_features, valid_labels), (train_features, train_labels)],\n",
    "                eval_names = ['valid', 'train'],\n",
    "                early_stopping_rounds = 100, verbose = 0)\n",
    "\n",
    "        # Record the best iteration\n",
    "        best_iteration = bst.best_iteration_\n",
    "\n",
    "        # Record the feature importances\n",
    "        feature_importance_values += bst.feature_importances_ / valid_splits\n",
    "        \n",
    "        # Make predictions\n",
    "        test_predictions += bst.predict_proba(test_features, num_iteration = best_iteration)[:, 1] / valid_splits\n",
    "\n",
    "        # Record the out of fold predictions\n",
    "        out_of_fold[valid_indices] = bst.predict_proba(valid_features, num_iteration = best_iteration)[:, 1]\n",
    "\n",
    "        # Record the best score\n",
    "        valid_score = bst.best_score_['valid']['auc']\n",
    "        train_score = bst.best_score_['train']['auc']\n",
    "        valid_acc = bst.best_score_['valid']['binary_error']\n",
    "        train_acc = bst.best_score_['train']['binary_error']\n",
    "        \n",
    "        valid_scores.append(valid_score)\n",
    "        train_scores.append(train_score)\n",
    "        valid_accs.append(valid_acc)\n",
    "        train_accs.append(train_acc)\n",
    "        \n",
    "    # Properly format feature importances\n",
    "    named_importances = list(zip(feature_names, feature_importance_values))\n",
    "    sorted_importances = sorted(named_importances, key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "    # Set up an exportable dictionary with results from the model\n",
    "    results = {\n",
    "        'train_auc':train_scores,\n",
    "        'train_acc':train_acc,\n",
    "        'validation_auc':valid_scores,\n",
    "        'validation_accuracy':valid_acc,\n",
    "        'valid_preds':out_of_fold,\n",
    "        'feature_importances':sorted_importances,\n",
    "        'test_predictions':test_predictions\n",
    "    }\n",
    "    \n",
    "    # Output the results dictionary\n",
    "    if export:\n",
    "        outpath = \"lgbc_{}.pickle\".format(ticker.tolower())\n",
    "        with open(outpath, 'wb') as f:\n",
    "            pickle.dump(results, f)\n",
    "    \n",
    "    print(\"Average AUC across {} splits: {}\".format(valid_splits, np.mean(valid_scores)))\n",
    "    print(\"Average accuracy across {} splits: {}%\".format(valid_splits, 100*np.round((1-np.mean(valid_acc)),2)))\n",
    "    for i in range(6):\n",
    "        print(sorted_importances[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
