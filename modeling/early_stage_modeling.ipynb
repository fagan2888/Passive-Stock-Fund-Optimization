{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Early-stage modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By: Jared Berry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for data preparation/EDA\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from scipy import stats\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up & data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure working directory is set appropriately - change as needed\n",
    "os.chdir('C:\\\\Users\\\\jared\\\\Documents\\\\data_science\\\\school\\\\georgetown\\\\capstone\\\\Passive-Stock-Fund-Optimization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets into memory\n",
    "yahoo = pd.read_csv('stock_price_until_2019_04_28.csv')\n",
    "drvd = pd.read_csv('moving-avg-momentum.csv')\n",
    "simfin = pd.read_csv('simfin\\daily_simfin.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yahoo! Finance data has 1014554 observations and 22 features.\n",
      "Derived data has 1014554 observations and 20 features.\n",
      "Daily SimFin data has 1509516 observations and 53 features.\n"
     ]
    }
   ],
   "source": [
    "# Check dimensions\n",
    "print(\"Yahoo! Finance data has {} observations and {} features.\".format(yahoo.shape[0], yahoo.shape[1]))\n",
    "print(\"Derived data has {} observations and {} features.\".format(drvd.shape[0], drvd.shape[1]))\n",
    "print(\"Daily SimFin data has {} observations and {} features.\".format(simfin.shape[0], simfin.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yahoo! Finance:\n",
      "  Symbol date_of_transaction\n",
      "0    ABT          2011-01-03\n",
      "1    ABT          2011-01-04\n",
      "2    ABT          2011-01-05\n",
      "3    ABT          2011-01-06\n",
      "4    ABT          2011-01-07\n",
      "Derived (Yahoo! Finance)\n",
      "  Symbol        Date\n",
      "0      A  2011-01-03\n",
      "1      A  2011-01-04\n",
      "2      A  2011-01-05\n",
      "3      A  2011-01-06\n",
      "4      A  2011-01-07\n",
      "SimFin:\n",
      "  ticker        date\n",
      "0    MMM  2011-03-31\n",
      "1    MMM  2011-04-01\n",
      "2    MMM  2011-04-02\n",
      "3    MMM  2011-04-03\n",
      "4    MMM  2011-04-04\n"
     ]
    }
   ],
   "source": [
    "# Check keys\n",
    "print(\"Yahoo! Finance:\")\n",
    "print(yahoo[['Symbol', 'date_of_transaction']].head())\n",
    "print(\"Derived (Yahoo! Finance)\")\n",
    "print(drvd[['Symbol', 'Date']].head())\n",
    "print(\"SimFin:\")\n",
    "print(simfin[['ticker', 'date']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some quick fixes on keys\n",
    "yahoo['ticker'] = yahoo['Symbol']\n",
    "yahoo.drop('Symbol', axis=1, inplace=True)\n",
    "\n",
    "drvd['ticker'] = drvd['Symbol']\n",
    "drvd['date_of_transaction'] = drvd['Date']\n",
    "drvd.drop(['Symbol', 'Date', 'High', 'Low', \n",
    "           'Open', 'Close', 'Volume', 'AdjClose'], \n",
    "          axis=1, inplace=True)\n",
    "\n",
    "simfin['date_of_transaction'] = simfin['date']\n",
    "simfin.drop('date', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sno</th>\n",
       "      <th>date_of_transaction</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>AdjClose</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>...</th>\n",
       "      <th>total_assets</th>\n",
       "      <th>total_current_assets</th>\n",
       "      <th>total_current_liabilities</th>\n",
       "      <th>total_equity</th>\n",
       "      <th>total_liabilities</th>\n",
       "      <th>total_liabilities_equity</th>\n",
       "      <th>total_noncurrent_assets</th>\n",
       "      <th>total_noncurrent_liabilities</th>\n",
       "      <th>common_outstanding_basic</th>\n",
       "      <th>common_outstanding_diluted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22334</th>\n",
       "      <td>22334</td>\n",
       "      <td>2011-01-03</td>\n",
       "      <td>30.143061</td>\n",
       "      <td>29.620888</td>\n",
       "      <td>29.728184</td>\n",
       "      <td>29.957081</td>\n",
       "      <td>4994000.0</td>\n",
       "      <td>27.591616</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22335</th>\n",
       "      <td>22335</td>\n",
       "      <td>2011-01-04</td>\n",
       "      <td>30.114449</td>\n",
       "      <td>29.456366</td>\n",
       "      <td>30.035765</td>\n",
       "      <td>29.678112</td>\n",
       "      <td>5017200.0</td>\n",
       "      <td>27.334681</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22336</th>\n",
       "      <td>22336</td>\n",
       "      <td>2011-01-05</td>\n",
       "      <td>29.849785</td>\n",
       "      <td>29.327610</td>\n",
       "      <td>29.513592</td>\n",
       "      <td>29.613733</td>\n",
       "      <td>4519000.0</td>\n",
       "      <td>27.275387</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22337</th>\n",
       "      <td>22337</td>\n",
       "      <td>2011-01-06</td>\n",
       "      <td>29.928469</td>\n",
       "      <td>29.477825</td>\n",
       "      <td>29.592276</td>\n",
       "      <td>29.670958</td>\n",
       "      <td>4699000.0</td>\n",
       "      <td>27.328091</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22338</th>\n",
       "      <td>22338</td>\n",
       "      <td>2011-01-07</td>\n",
       "      <td>29.899857</td>\n",
       "      <td>29.356224</td>\n",
       "      <td>29.699572</td>\n",
       "      <td>29.771101</td>\n",
       "      <td>3810900.0</td>\n",
       "      <td>27.420322</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 84 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sno date_of_transaction       High        Low       Open      Close  \\\n",
       "22334  22334          2011-01-03  30.143061  29.620888  29.728184  29.957081   \n",
       "22335  22335          2011-01-04  30.114449  29.456366  30.035765  29.678112   \n",
       "22336  22336          2011-01-05  29.849785  29.327610  29.513592  29.613733   \n",
       "22337  22337          2011-01-06  29.928469  29.477825  29.592276  29.670958   \n",
       "22338  22338          2011-01-07  29.899857  29.356224  29.699572  29.771101   \n",
       "\n",
       "          Volume   AdjClose  Year  Month  ...  total_assets  \\\n",
       "22334  4994000.0  27.591616  2011      1  ...           NaN   \n",
       "22335  5017200.0  27.334681  2011      1  ...           NaN   \n",
       "22336  4519000.0  27.275387  2011      1  ...           NaN   \n",
       "22337  4699000.0  27.328091  2011      1  ...           NaN   \n",
       "22338  3810900.0  27.420322  2011      1  ...           NaN   \n",
       "\n",
       "       total_current_assets  total_current_liabilities  total_equity  \\\n",
       "22334                   NaN                        NaN           NaN   \n",
       "22335                   NaN                        NaN           NaN   \n",
       "22336                   NaN                        NaN           NaN   \n",
       "22337                   NaN                        NaN           NaN   \n",
       "22338                   NaN                        NaN           NaN   \n",
       "\n",
       "       total_liabilities  total_liabilities_equity  total_noncurrent_assets  \\\n",
       "22334                NaN                       NaN                      NaN   \n",
       "22335                NaN                       NaN                      NaN   \n",
       "22336                NaN                       NaN                      NaN   \n",
       "22337                NaN                       NaN                      NaN   \n",
       "22338                NaN                       NaN                      NaN   \n",
       "\n",
       "       total_noncurrent_liabilities  common_outstanding_basic  \\\n",
       "22334                           NaN                       NaN   \n",
       "22335                           NaN                       NaN   \n",
       "22336                           NaN                       NaN   \n",
       "22337                           NaN                       NaN   \n",
       "22338                           NaN                       NaN   \n",
       "\n",
       "       common_outstanding_diluted  \n",
       "22334                         NaN  \n",
       "22335                         NaN  \n",
       "22336                         NaN  \n",
       "22337                         NaN  \n",
       "22338                         NaN  \n",
       "\n",
       "[5 rows x 84 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct the 'train' dataset by merging stock prices and fundamentals; ensure proper sorting and filter on early sample\n",
    "train = pd.merge(yahoo, drvd, on=['ticker', 'date_of_transaction'])\n",
    "train = pd.merge(train, simfin, how='left', on=['ticker', 'date_of_transaction'])\n",
    "\n",
    "train = train.sort_values(['ticker','date_of_transaction'])\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct some aggregate financial ratios from the SimFin data\n",
    "train['eps'] = train['net_income_y'] / train['common_outstanding_basic']\n",
    "train['pe_ratio'] = train['AdjClose'] / train['eps']\n",
    "train['debt_ratio'] = train['total_liabilities'] / train['total_equity']\n",
    "train['debt_to_equity'] = train['total_liabilities'] / train['total_equity']\n",
    "train['roa'] = train['net_income_y'] / train['total_assets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct some additional ticker-level returns features\n",
    "train['open_l1'] = train.groupby('ticker')['Open'].shift(1)\n",
    "train['open_l5'] = train.groupby('ticker')['Open'].shift(5)\n",
    "train['open_l10'] = train.groupby('ticker')['Open'].shift(10)\n",
    "\n",
    "train['return_prev1_open_raw'] = 100*(train['Open'] - train['open_l1'])/train['open_l1']\n",
    "train['return_prev5_open_raw'] = 100*(train['Open'] - train['open_l5'])/train['open_l5']\n",
    "train['return_prev10_open_raw'] = 100*(train['Open'] - train['open_l10'])/train['open_l10']\n",
    "\n",
    "train['close_l1'] = train.groupby('ticker')['AdjClose'].shift(1)\n",
    "train['close_l5'] = train.groupby('ticker')['AdjClose'].shift(5)\n",
    "train['close_l10'] = train.groupby('ticker')['AdjClose'].shift(10)\n",
    "\n",
    "train['return_prev1_close_raw'] = 100*(train['AdjClose'] - train['close_l1'])/train['close_l1']\n",
    "train['return_prev5_close_raw'] = 100*(train['AdjClose'] - train['close_l5'])/train['close_l5']\n",
    "train['return_prev10_close_raw'] = 100*(train['AdjClose'] - train['close_l10'])/train['close_l10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sno</th>\n",
       "      <th>date_of_transaction</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>AdjClose</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>...</th>\n",
       "      <th>open_l10</th>\n",
       "      <th>return_prev1_open_raw</th>\n",
       "      <th>return_prev5_open_raw</th>\n",
       "      <th>return_prev10_open_raw</th>\n",
       "      <th>close_l1</th>\n",
       "      <th>close_l5</th>\n",
       "      <th>close_l10</th>\n",
       "      <th>return_prev1_close_raw</th>\n",
       "      <th>return_prev5_close_raw</th>\n",
       "      <th>return_prev10_close_raw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>98422</th>\n",
       "      <td>98422</td>\n",
       "      <td>2011-01-03</td>\n",
       "      <td>47.180000</td>\n",
       "      <td>46.405716</td>\n",
       "      <td>46.520000</td>\n",
       "      <td>47.081429</td>\n",
       "      <td>111284600.0</td>\n",
       "      <td>31.394041</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98423</th>\n",
       "      <td>98423</td>\n",
       "      <td>2011-01-04</td>\n",
       "      <td>47.500000</td>\n",
       "      <td>46.878571</td>\n",
       "      <td>47.491428</td>\n",
       "      <td>47.327145</td>\n",
       "      <td>77270200.0</td>\n",
       "      <td>31.557884</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.088194</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.394041</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.521893</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98424</th>\n",
       "      <td>98424</td>\n",
       "      <td>2011-01-05</td>\n",
       "      <td>47.762856</td>\n",
       "      <td>47.071430</td>\n",
       "      <td>47.078571</td>\n",
       "      <td>47.714287</td>\n",
       "      <td>63879900.0</td>\n",
       "      <td>31.816027</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.869330</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.557884</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.817997</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98425</th>\n",
       "      <td>98425</td>\n",
       "      <td>2011-01-06</td>\n",
       "      <td>47.892857</td>\n",
       "      <td>47.557144</td>\n",
       "      <td>47.817142</td>\n",
       "      <td>47.675713</td>\n",
       "      <td>75107200.0</td>\n",
       "      <td>31.790306</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.568805</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.816027</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.080842</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98426</th>\n",
       "      <td>98426</td>\n",
       "      <td>2011-01-07</td>\n",
       "      <td>48.049999</td>\n",
       "      <td>47.414288</td>\n",
       "      <td>47.712856</td>\n",
       "      <td>48.017143</td>\n",
       "      <td>77982800.0</td>\n",
       "      <td>32.017971</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.218094</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.790306</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.716146</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98427</th>\n",
       "      <td>98427</td>\n",
       "      <td>2011-01-10</td>\n",
       "      <td>49.032856</td>\n",
       "      <td>48.167141</td>\n",
       "      <td>48.404285</td>\n",
       "      <td>48.921429</td>\n",
       "      <td>112140000.0</td>\n",
       "      <td>32.620960</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.449146</td>\n",
       "      <td>4.050484</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.017971</td>\n",
       "      <td>31.394041</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.883284</td>\n",
       "      <td>3.908128</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98428</th>\n",
       "      <td>98428</td>\n",
       "      <td>2011-01-11</td>\n",
       "      <td>49.279999</td>\n",
       "      <td>48.495716</td>\n",
       "      <td>49.268570</td>\n",
       "      <td>48.805714</td>\n",
       "      <td>111027000.0</td>\n",
       "      <td>32.543804</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.785554</td>\n",
       "      <td>3.742026</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.620960</td>\n",
       "      <td>31.557884</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.236523</td>\n",
       "      <td>3.124164</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98429</th>\n",
       "      <td>98429</td>\n",
       "      <td>2011-01-12</td>\n",
       "      <td>49.204285</td>\n",
       "      <td>48.857143</td>\n",
       "      <td>49.035713</td>\n",
       "      <td>49.202858</td>\n",
       "      <td>75647600.0</td>\n",
       "      <td>32.808609</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.472627</td>\n",
       "      <td>4.157182</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.543804</td>\n",
       "      <td>31.816027</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.813687</td>\n",
       "      <td>3.119756</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98430</th>\n",
       "      <td>98430</td>\n",
       "      <td>2011-01-13</td>\n",
       "      <td>49.520000</td>\n",
       "      <td>49.121429</td>\n",
       "      <td>49.308571</td>\n",
       "      <td>49.382858</td>\n",
       "      <td>74195100.0</td>\n",
       "      <td>32.928646</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.556447</td>\n",
       "      <td>3.119024</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.808609</td>\n",
       "      <td>31.790306</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.365871</td>\n",
       "      <td>3.580777</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98431</th>\n",
       "      <td>98431</td>\n",
       "      <td>2011-01-14</td>\n",
       "      <td>49.782856</td>\n",
       "      <td>49.205715</td>\n",
       "      <td>49.412857</td>\n",
       "      <td>49.782856</td>\n",
       "      <td>77210000.0</td>\n",
       "      <td>33.195358</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.211497</td>\n",
       "      <td>3.562983</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.928646</td>\n",
       "      <td>32.017971</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.809970</td>\n",
       "      <td>3.677270</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98432</th>\n",
       "      <td>98432</td>\n",
       "      <td>2011-01-18</td>\n",
       "      <td>49.251427</td>\n",
       "      <td>46.571430</td>\n",
       "      <td>47.074287</td>\n",
       "      <td>48.664288</td>\n",
       "      <td>470249500.0</td>\n",
       "      <td>32.449497</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>46.520000</td>\n",
       "      <td>-4.732715</td>\n",
       "      <td>-2.747687</td>\n",
       "      <td>1.191502</td>\n",
       "      <td>33.195358</td>\n",
       "      <td>32.620960</td>\n",
       "      <td>31.394041</td>\n",
       "      <td>-2.246884</td>\n",
       "      <td>-0.525622</td>\n",
       "      <td>3.361963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98433</th>\n",
       "      <td>98433</td>\n",
       "      <td>2011-01-19</td>\n",
       "      <td>49.799999</td>\n",
       "      <td>48.125713</td>\n",
       "      <td>49.764286</td>\n",
       "      <td>48.405716</td>\n",
       "      <td>283903200.0</td>\n",
       "      <td>32.277077</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>47.491428</td>\n",
       "      <td>5.714369</td>\n",
       "      <td>1.006151</td>\n",
       "      <td>4.785827</td>\n",
       "      <td>32.449497</td>\n",
       "      <td>32.543804</td>\n",
       "      <td>31.557884</td>\n",
       "      <td>-0.531350</td>\n",
       "      <td>-0.819595</td>\n",
       "      <td>2.278963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98434</th>\n",
       "      <td>98434</td>\n",
       "      <td>2011-01-20</td>\n",
       "      <td>48.328571</td>\n",
       "      <td>47.160000</td>\n",
       "      <td>48.061428</td>\n",
       "      <td>47.525715</td>\n",
       "      <td>191197300.0</td>\n",
       "      <td>31.690289</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>47.078571</td>\n",
       "      <td>-3.421847</td>\n",
       "      <td>-1.986889</td>\n",
       "      <td>2.087695</td>\n",
       "      <td>32.277077</td>\n",
       "      <td>32.808609</td>\n",
       "      <td>31.816027</td>\n",
       "      <td>-1.817972</td>\n",
       "      <td>-3.408619</td>\n",
       "      <td>-0.395204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98435</th>\n",
       "      <td>98435</td>\n",
       "      <td>2011-01-21</td>\n",
       "      <td>47.840000</td>\n",
       "      <td>46.661430</td>\n",
       "      <td>47.681427</td>\n",
       "      <td>46.674286</td>\n",
       "      <td>188600300.0</td>\n",
       "      <td>31.122555</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>47.817142</td>\n",
       "      <td>-0.790657</td>\n",
       "      <td>-3.299921</td>\n",
       "      <td>-0.283822</td>\n",
       "      <td>31.690289</td>\n",
       "      <td>32.928646</td>\n",
       "      <td>31.790306</td>\n",
       "      <td>-1.791507</td>\n",
       "      <td>-5.484864</td>\n",
       "      <td>-2.100487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98436</th>\n",
       "      <td>98436</td>\n",
       "      <td>2011-01-24</td>\n",
       "      <td>48.207142</td>\n",
       "      <td>46.674286</td>\n",
       "      <td>46.695713</td>\n",
       "      <td>48.207142</td>\n",
       "      <td>143670800.0</td>\n",
       "      <td>32.144680</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>47.712856</td>\n",
       "      <td>-2.067291</td>\n",
       "      <td>-5.498860</td>\n",
       "      <td>-2.131801</td>\n",
       "      <td>31.122555</td>\n",
       "      <td>33.195358</td>\n",
       "      <td>32.017971</td>\n",
       "      <td>3.284195</td>\n",
       "      <td>-3.165136</td>\n",
       "      <td>0.395743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98437</th>\n",
       "      <td>98437</td>\n",
       "      <td>2011-01-25</td>\n",
       "      <td>48.777142</td>\n",
       "      <td>47.795715</td>\n",
       "      <td>48.047142</td>\n",
       "      <td>48.771427</td>\n",
       "      <td>136717000.0</td>\n",
       "      <td>32.520935</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>48.404285</td>\n",
       "      <td>2.894118</td>\n",
       "      <td>2.066637</td>\n",
       "      <td>-0.737834</td>\n",
       "      <td>32.144680</td>\n",
       "      <td>32.449497</td>\n",
       "      <td>32.620960</td>\n",
       "      <td>1.170505</td>\n",
       "      <td>0.220151</td>\n",
       "      <td>-0.306629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98438</th>\n",
       "      <td>98438</td>\n",
       "      <td>2011-01-26</td>\n",
       "      <td>49.371429</td>\n",
       "      <td>48.785713</td>\n",
       "      <td>48.994286</td>\n",
       "      <td>49.121429</td>\n",
       "      <td>126718900.0</td>\n",
       "      <td>32.754322</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>49.268570</td>\n",
       "      <td>1.971280</td>\n",
       "      <td>-1.547295</td>\n",
       "      <td>-0.556713</td>\n",
       "      <td>32.520935</td>\n",
       "      <td>32.277077</td>\n",
       "      <td>32.543804</td>\n",
       "      <td>0.717652</td>\n",
       "      <td>1.478589</td>\n",
       "      <td>0.646875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98439</th>\n",
       "      <td>98439</td>\n",
       "      <td>2011-01-27</td>\n",
       "      <td>49.241428</td>\n",
       "      <td>48.975716</td>\n",
       "      <td>49.111427</td>\n",
       "      <td>49.029999</td>\n",
       "      <td>71256500.0</td>\n",
       "      <td>32.693359</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>49.035713</td>\n",
       "      <td>0.239093</td>\n",
       "      <td>2.184703</td>\n",
       "      <td>0.154406</td>\n",
       "      <td>32.754322</td>\n",
       "      <td>31.690289</td>\n",
       "      <td>32.808609</td>\n",
       "      <td>-0.186121</td>\n",
       "      <td>3.165231</td>\n",
       "      <td>-0.351279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98440</th>\n",
       "      <td>98440</td>\n",
       "      <td>2011-01-28</td>\n",
       "      <td>49.200001</td>\n",
       "      <td>47.647144</td>\n",
       "      <td>49.167141</td>\n",
       "      <td>48.014286</td>\n",
       "      <td>148014300.0</td>\n",
       "      <td>32.016071</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>49.308571</td>\n",
       "      <td>0.113443</td>\n",
       "      <td>3.115918</td>\n",
       "      <td>-0.286826</td>\n",
       "      <td>32.693359</td>\n",
       "      <td>31.122555</td>\n",
       "      <td>32.928646</td>\n",
       "      <td>-2.071638</td>\n",
       "      <td>2.870961</td>\n",
       "      <td>-2.771370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98441</th>\n",
       "      <td>98441</td>\n",
       "      <td>2011-01-31</td>\n",
       "      <td>48.577145</td>\n",
       "      <td>47.757141</td>\n",
       "      <td>47.971428</td>\n",
       "      <td>48.474285</td>\n",
       "      <td>94311700.0</td>\n",
       "      <td>32.322800</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>49.412857</td>\n",
       "      <td>-2.431935</td>\n",
       "      <td>2.731974</td>\n",
       "      <td>-2.917114</td>\n",
       "      <td>32.016071</td>\n",
       "      <td>32.144680</td>\n",
       "      <td>33.195358</td>\n",
       "      <td>0.958045</td>\n",
       "      <td>0.554119</td>\n",
       "      <td>-2.628556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98442</th>\n",
       "      <td>98442</td>\n",
       "      <td>2011-02-01</td>\n",
       "      <td>49.378571</td>\n",
       "      <td>48.711430</td>\n",
       "      <td>48.757141</td>\n",
       "      <td>49.290001</td>\n",
       "      <td>106658300.0</td>\n",
       "      <td>32.866718</td>\n",
       "      <td>2011</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>47.074287</td>\n",
       "      <td>1.637877</td>\n",
       "      <td>1.477713</td>\n",
       "      <td>3.574889</td>\n",
       "      <td>32.322800</td>\n",
       "      <td>32.520935</td>\n",
       "      <td>32.449497</td>\n",
       "      <td>1.682771</td>\n",
       "      <td>1.063264</td>\n",
       "      <td>1.285755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98443</th>\n",
       "      <td>98443</td>\n",
       "      <td>2011-02-02</td>\n",
       "      <td>49.321430</td>\n",
       "      <td>49.078571</td>\n",
       "      <td>49.207142</td>\n",
       "      <td>49.188572</td>\n",
       "      <td>64738800.0</td>\n",
       "      <td>32.799084</td>\n",
       "      <td>2011</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>49.764286</td>\n",
       "      <td>0.922943</td>\n",
       "      <td>0.434451</td>\n",
       "      <td>-1.119566</td>\n",
       "      <td>32.866718</td>\n",
       "      <td>32.754322</td>\n",
       "      <td>32.277077</td>\n",
       "      <td>-0.205784</td>\n",
       "      <td>0.136659</td>\n",
       "      <td>1.617268</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sno date_of_transaction       High        Low       Open      Close  \\\n",
       "98422  98422          2011-01-03  47.180000  46.405716  46.520000  47.081429   \n",
       "98423  98423          2011-01-04  47.500000  46.878571  47.491428  47.327145   \n",
       "98424  98424          2011-01-05  47.762856  47.071430  47.078571  47.714287   \n",
       "98425  98425          2011-01-06  47.892857  47.557144  47.817142  47.675713   \n",
       "98426  98426          2011-01-07  48.049999  47.414288  47.712856  48.017143   \n",
       "98427  98427          2011-01-10  49.032856  48.167141  48.404285  48.921429   \n",
       "98428  98428          2011-01-11  49.279999  48.495716  49.268570  48.805714   \n",
       "98429  98429          2011-01-12  49.204285  48.857143  49.035713  49.202858   \n",
       "98430  98430          2011-01-13  49.520000  49.121429  49.308571  49.382858   \n",
       "98431  98431          2011-01-14  49.782856  49.205715  49.412857  49.782856   \n",
       "98432  98432          2011-01-18  49.251427  46.571430  47.074287  48.664288   \n",
       "98433  98433          2011-01-19  49.799999  48.125713  49.764286  48.405716   \n",
       "98434  98434          2011-01-20  48.328571  47.160000  48.061428  47.525715   \n",
       "98435  98435          2011-01-21  47.840000  46.661430  47.681427  46.674286   \n",
       "98436  98436          2011-01-24  48.207142  46.674286  46.695713  48.207142   \n",
       "98437  98437          2011-01-25  48.777142  47.795715  48.047142  48.771427   \n",
       "98438  98438          2011-01-26  49.371429  48.785713  48.994286  49.121429   \n",
       "98439  98439          2011-01-27  49.241428  48.975716  49.111427  49.029999   \n",
       "98440  98440          2011-01-28  49.200001  47.647144  49.167141  48.014286   \n",
       "98441  98441          2011-01-31  48.577145  47.757141  47.971428  48.474285   \n",
       "98442  98442          2011-02-01  49.378571  48.711430  48.757141  49.290001   \n",
       "98443  98443          2011-02-02  49.321430  49.078571  49.207142  49.188572   \n",
       "\n",
       "            Volume   AdjClose  Year  Month  ...   open_l10  \\\n",
       "98422  111284600.0  31.394041  2011      1  ...        NaN   \n",
       "98423   77270200.0  31.557884  2011      1  ...        NaN   \n",
       "98424   63879900.0  31.816027  2011      1  ...        NaN   \n",
       "98425   75107200.0  31.790306  2011      1  ...        NaN   \n",
       "98426   77982800.0  32.017971  2011      1  ...        NaN   \n",
       "98427  112140000.0  32.620960  2011      1  ...        NaN   \n",
       "98428  111027000.0  32.543804  2011      1  ...        NaN   \n",
       "98429   75647600.0  32.808609  2011      1  ...        NaN   \n",
       "98430   74195100.0  32.928646  2011      1  ...        NaN   \n",
       "98431   77210000.0  33.195358  2011      1  ...        NaN   \n",
       "98432  470249500.0  32.449497  2011      1  ...  46.520000   \n",
       "98433  283903200.0  32.277077  2011      1  ...  47.491428   \n",
       "98434  191197300.0  31.690289  2011      1  ...  47.078571   \n",
       "98435  188600300.0  31.122555  2011      1  ...  47.817142   \n",
       "98436  143670800.0  32.144680  2011      1  ...  47.712856   \n",
       "98437  136717000.0  32.520935  2011      1  ...  48.404285   \n",
       "98438  126718900.0  32.754322  2011      1  ...  49.268570   \n",
       "98439   71256500.0  32.693359  2011      1  ...  49.035713   \n",
       "98440  148014300.0  32.016071  2011      1  ...  49.308571   \n",
       "98441   94311700.0  32.322800  2011      1  ...  49.412857   \n",
       "98442  106658300.0  32.866718  2011      2  ...  47.074287   \n",
       "98443   64738800.0  32.799084  2011      2  ...  49.764286   \n",
       "\n",
       "       return_prev1_open_raw  return_prev5_open_raw  return_prev10_open_raw  \\\n",
       "98422                    NaN                    NaN                     NaN   \n",
       "98423               2.088194                    NaN                     NaN   \n",
       "98424              -0.869330                    NaN                     NaN   \n",
       "98425               1.568805                    NaN                     NaN   \n",
       "98426              -0.218094                    NaN                     NaN   \n",
       "98427               1.449146               4.050484                     NaN   \n",
       "98428               1.785554               3.742026                     NaN   \n",
       "98429              -0.472627               4.157182                     NaN   \n",
       "98430               0.556447               3.119024                     NaN   \n",
       "98431               0.211497               3.562983                     NaN   \n",
       "98432              -4.732715              -2.747687                1.191502   \n",
       "98433               5.714369               1.006151                4.785827   \n",
       "98434              -3.421847              -1.986889                2.087695   \n",
       "98435              -0.790657              -3.299921               -0.283822   \n",
       "98436              -2.067291              -5.498860               -2.131801   \n",
       "98437               2.894118               2.066637               -0.737834   \n",
       "98438               1.971280              -1.547295               -0.556713   \n",
       "98439               0.239093               2.184703                0.154406   \n",
       "98440               0.113443               3.115918               -0.286826   \n",
       "98441              -2.431935               2.731974               -2.917114   \n",
       "98442               1.637877               1.477713                3.574889   \n",
       "98443               0.922943               0.434451               -1.119566   \n",
       "\n",
       "        close_l1   close_l5  close_l10  return_prev1_close_raw  \\\n",
       "98422        NaN        NaN        NaN                     NaN   \n",
       "98423  31.394041        NaN        NaN                0.521893   \n",
       "98424  31.557884        NaN        NaN                0.817997   \n",
       "98425  31.816027        NaN        NaN               -0.080842   \n",
       "98426  31.790306        NaN        NaN                0.716146   \n",
       "98427  32.017971  31.394041        NaN                1.883284   \n",
       "98428  32.620960  31.557884        NaN               -0.236523   \n",
       "98429  32.543804  31.816027        NaN                0.813687   \n",
       "98430  32.808609  31.790306        NaN                0.365871   \n",
       "98431  32.928646  32.017971        NaN                0.809970   \n",
       "98432  33.195358  32.620960  31.394041               -2.246884   \n",
       "98433  32.449497  32.543804  31.557884               -0.531350   \n",
       "98434  32.277077  32.808609  31.816027               -1.817972   \n",
       "98435  31.690289  32.928646  31.790306               -1.791507   \n",
       "98436  31.122555  33.195358  32.017971                3.284195   \n",
       "98437  32.144680  32.449497  32.620960                1.170505   \n",
       "98438  32.520935  32.277077  32.543804                0.717652   \n",
       "98439  32.754322  31.690289  32.808609               -0.186121   \n",
       "98440  32.693359  31.122555  32.928646               -2.071638   \n",
       "98441  32.016071  32.144680  33.195358                0.958045   \n",
       "98442  32.322800  32.520935  32.449497                1.682771   \n",
       "98443  32.866718  32.754322  32.277077               -0.205784   \n",
       "\n",
       "       return_prev5_close_raw  return_prev10_close_raw  \n",
       "98422                     NaN                      NaN  \n",
       "98423                     NaN                      NaN  \n",
       "98424                     NaN                      NaN  \n",
       "98425                     NaN                      NaN  \n",
       "98426                     NaN                      NaN  \n",
       "98427                3.908128                      NaN  \n",
       "98428                3.124164                      NaN  \n",
       "98429                3.119756                      NaN  \n",
       "98430                3.580777                      NaN  \n",
       "98431                3.677270                      NaN  \n",
       "98432               -0.525622                 3.361963  \n",
       "98433               -0.819595                 2.278963  \n",
       "98434               -3.408619                -0.395204  \n",
       "98435               -5.484864                -2.100487  \n",
       "98436               -3.165136                 0.395743  \n",
       "98437                0.220151                -0.306629  \n",
       "98438                1.478589                 0.646875  \n",
       "98439                3.165231                -0.351279  \n",
       "98440                2.870961                -2.771370  \n",
       "98441                0.554119                -2.628556  \n",
       "98442                1.063264                 1.285755  \n",
       "98443                0.136659                 1.617268  \n",
       "\n",
       "[22 rows x 101 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[train['ticker'] == 'AAPL'].iloc[0:22,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the quarter of pre-SimFin data\n",
    "train = train[train['date_of_transaction'] >= '2011-03-31'].reset_index().drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select on some tickers for \n",
    "train = train[train['ticker'].isin(['JPM', 'WFC', 'A', 'AAPL', 'GOOG'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify ranges\n",
    "n = 21 # n-day ahead return\n",
    "q = 1 # q-day window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At the ticker level, lead the AdjClose column by n-trading days\n",
    "target_gen = train[['ticker', 'date_of_transaction', 'AdjClose']]\n",
    "AdjClose_ahead = target_gen.groupby('ticker')['AdjClose'].shift(-n)\n",
    "AdjClose_ahead.name = 'AdjClose_ahead'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw, month-ahead return is calculated as:\n",
    "$$target_{t,i} = \\frac{AdjClose_{t+n,i} - AdjClose_{t,i}}{AdjClose_{t,i}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_raw = np.array(100*((AdjClose_ahead - train['AdjClose'])/train['AdjClose']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing all of the returns for the next 21 days (month) relative to today\n",
    "aheads = []\n",
    "for i in range(0,n+1):\n",
    "    AdjClose_ahead_i = target_gen.groupby('ticker')['AdjClose'].shift(-i)\n",
    "    aheads.append(np.array(100*((AdjClose_ahead_i - train['AdjClose'])/train['AdjClose'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average, raw returns for all periods within the next month (relative to today) is calculated as:\n",
    "$$target_{t,i} = (\\frac{1}{n})\\sum_{k=1}^n \\frac{AdjClose_{t+k,i} - AdjClose_{t,i}}{AdjClose_{t,i}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cma = np.array(pd.DataFrame(aheads).mean(axis=0, skipna=False).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The w-day moving average of n-day ahead raw returns, relative to today is calculated as:\n",
    "$$target_{t,i,q} = \\frac{AdjClose_{t+n,i} - AdjClose_{t,i}}{AdjClose_{t,i}}, MA(q)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Simpler moving-average ahead target?\n",
    "target_gen['returns_ahead'] = 100*((AdjClose_ahead - train['AdjClose'])/train['AdjClose'])\n",
    "target_ma = np.array(target_gen.groupby('ticker')['returns_ahead'].rolling(q).mean()) # NEED TO CHECK THE LAG STRUCTURE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct market-residualized variants (NEED S&P 500 OR CONSTRUCTION OF AN INDEX FROM CURRENT FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 100 target\n",
    "target_rank = train.groupby('ticker')['Monthly_Return_Rank'].shift(-n)\n",
    "target_rank = np.where(np.isnan(target_rank), np.nan,\n",
    "              np.where(target_rank < 100, 1, 0))\n",
    "target_rank = target_rank.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Up target\n",
    "target_up = np.where(np.isnan(AdjClose_ahead), np.nan,\n",
    "            np.where(AdjClose_ahead > train['AdjClose'], 1, 0))\n",
    "target_up = target_up.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporatory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6066198595787362"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.nanmean(target_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18986960882647944"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.nanmean(target_rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a feature selection list (THINK ABOUT INFORMING THIS SELECTION WITH SHRINKAGE METHODS, I.E. RIDGE REGRESSION)\n",
    "features = ['High', 'Low', 'Open', 'Close', 'Volume', 'AdjClose', 'Year',\n",
    "            'Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear', 'Pct_Change_Daily',\n",
    "            'Pct_Change_Monthly', 'Pct_Change_Yearly', 'RSI', 'Volatility',\n",
    "            'Yearly_Return_Rank', 'Monthly_Return_Rank', 'Pct_Change_Class',\n",
    "            'Rolling_Yearly_Mean_Positive_Days', 'Rolling_Monthly_Mean_Positive_Days', \n",
    "            'Rolling_Monthly_Mean_Price', 'Rolling_Yearly_Mean_Price',\n",
    "            'open_l1', 'open_l5', 'open_l10', 'close_l1', 'close_l5', 'close_l10',\n",
    "            'return_prev1_open_raw', 'return_prev5_open_raw', 'return_prev10_open_raw',\n",
    "            'return_prev1_close_raw', 'return_prev5_close_raw', 'return_prev10_close_raw',\n",
    "            'pe_ratio', 'debt_ratio', 'debt_to_equity', 'roa'\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select on features to pass to modeling machinery\n",
    "X = train[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of the tickers\n",
    "tickers = train['ticker'].unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set relevant scikit-learn functions/modules\n",
    "\n",
    "# Tests for stationarity \n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Regression models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Classification models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# LightGBM\n",
    "from lightgbm import LGBMClassifier\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# Model selection\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Evaluation\n",
    "from sklearn import metrics\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Wheat Classification notebook \n",
    "def fit_sklearn_classifier(X, y, holdout, ticker, ema_gamma, valid_splits, model, label, param_search={}, export=False,\n",
    "                           valid_method=\"ts\", labeled=False, groups=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Flexible function for fitting any number of sci-kit learn\n",
    "    classifiers, with optional grid search.\n",
    "    \"\"\"\n",
    "    warnings.filterwarnings('always')\n",
    "    start = time.time()\n",
    "    # Convert to NumPy arrays - store feature names\n",
    "    feature_names = X.columns.tolist()\n",
    "    features = np.array(X)\n",
    "    test_features = np.array(holdout)\n",
    "    labels = y.copy()\n",
    "    \n",
    "    if labeled:\n",
    "        labels_smoothed = labels.copy()\n",
    "        orig_lables = labels.copy()\n",
    "    else:\n",
    "        # Compute EMA smoothing of target prior to constructing classes\n",
    "        EMA = 0\n",
    "        gamma_ = ema_gamma\n",
    "        for ti in range(len(labels)):\n",
    "            EMA = gamma_*labels[ti] + (1-gamma_)*EMA\n",
    "            labels[ti] = EMA  \n",
    "\n",
    "        labels_smoothed = np.where(labels > 0, 1, 0)    \n",
    "        orig_labels = np.where(y.copy() > 0, 1, 0)\n",
    "        print(\"\\n{} labels changed by smoothing.\".format(np.sum(labels_smoothed != orig_labels)))\n",
    "\n",
    "    # Compute some baselines\n",
    "    all_pos_acc = np.mean(np.ones(test_features.shape[0], dtype='int') == labels_smoothed)\n",
    "    random_walk_classes = np.array(pd.Series(labels_smoothed).shift(1).tolist()[1:], dtype=\"int\") # Inappropriate for panel\n",
    "    rw_acc = np.mean(random_walk_classes == np.array(labels_smoothed[1:], dtype=\"int\"))\n",
    "    print(\"Baseline accuracy is: {}%\".format(100*np.round(all_pos_acc, 2)))\n",
    "    print(\"RW accuracy is {}%\".format(100*np.round(rw_acc, 2)))\n",
    "    \n",
    "    # Set time-series, cross-validation indices\n",
    "    if valid_method == \"panel\":\n",
    "        splits = PanelSplit(n_folds=valid_splits, groups=groups)\n",
    "        search_splits = PanelSplit(n_folds=valid_splits, groups=groups)\n",
    "    elif valid_method == \"ts\":\n",
    "        splits = TimeSeriesSplit(n_splits=valid_splits).split(X)\n",
    "        search_splits = TimeSeriesSplit(n_splits=valid_splits).split(X)\n",
    "    elif valid_method == \"kfold\":\n",
    "        splits = KFold(n_splits = valid_splits).split(X)\n",
    "        search_splits = Kfold(n_splits = valid_splits).split(X)\n",
    "    \n",
    "    # Empty array for feature importances\n",
    "    feature_importance_values = np.zeros(len(feature_names))\n",
    "\n",
    "    # Empty array for out of fold validation predictions\n",
    "    out_of_fold = np.zeros(features.shape[0])\n",
    "    \n",
    "    # Empty array for test predictions\n",
    "    test_predictions = np.zeros(test_features.shape[0])\n",
    "    \n",
    "    # Dictionary of lists for recording validation and training scores\n",
    "    scores = {'precision':[], 'recall':[], 'accuracy':[], 'f1':[]}\n",
    "    \n",
    "    # Perform a grid-search on the provided parameters to determine best options\n",
    "    if param_search:\n",
    "        print(\"Performing grid search for hyperparameter tuning\")\n",
    "        gridsearch = GridSearchCV(estimator=model(), \n",
    "                                  cv=search_splits,\n",
    "                                  param_grid=param_search)\n",
    "\n",
    "        # Fit to extract best parameters later\n",
    "        gridsearch_model = gridsearch.fit(features, labels_smoothed)\n",
    "\n",
    "    opt_thresholds = []; split_counter = 1\n",
    "    for train_indices, valid_indices in splits:\n",
    "        print(\"Training model on validation split #{}\".format(split_counter))\n",
    "        # Training data for the fold\n",
    "        train_features, train_labels = features[train_indices], labels_smoothed[train_indices]\n",
    "        #Validation data for the fold\n",
    "        valid_features, valid_labels = features[valid_indices], labels_smoothed[valid_indices]\n",
    "        \n",
    "        expected = valid_labels\n",
    "        \n",
    "        # Generate a model given the optimal parameters established in grid search\n",
    "        if param_search:\n",
    "            estimator = model(**gridsearch_model.best_params_)\n",
    "        else:\n",
    "            estimator = model(**kwargs)\n",
    "            \n",
    "        # Train the estimator\n",
    "        estimator.fit(train_features, train_labels)\n",
    "\n",
    "        # Fit the fitted model on the test set and store positive class probabilities\n",
    "        probs = estimator.predict_proba(valid_features)\n",
    "        pos_probs = [p[1] for p in probs]\n",
    "\n",
    "        # Dynamic classification threshold selection\n",
    "        thresholds = list(np.arange(0.30, 0.90, 0.05))\n",
    "        preds = [[1 if y >= t else 0 for y in pos_probs] for t in thresholds]\n",
    "        scores_by_threshold_ = [metrics.f1_score(valid_labels, p) for p in preds]\n",
    "        opt_thresh_ = thresholds[scores_by_threshold_.index(max(scores_by_threshold_))]\n",
    "        opt_thresholds.append(opt_thresh_)\n",
    "\n",
    "        # Generate class predictions\n",
    "        predicted = [1 if y >= opt_thresh_ else 0 for y in pos_probs]\n",
    "\n",
    "        # Append scores to the tracker\n",
    "        scores['precision'].append(metrics.precision_score(expected, predicted, average=\"weighted\"))\n",
    "        scores['recall'].append(metrics.recall_score(expected, predicted, average=\"weighted\"))\n",
    "        scores['accuracy'].append(metrics.accuracy_score(expected, predicted))\n",
    "        scores['f1'].append(metrics.f1_score(expected, predicted, average=\"weighted\"))\n",
    "        \n",
    "        # Store variable importances \n",
    "        if model in [RandomForestClassifier, GradientBoostingClassifier]:\n",
    "            feature_importance_values += estimator.feature_importances_ / valid_splits\n",
    "            \n",
    "        # Iterate counter\n",
    "        split_counter += 1\n",
    "            \n",
    "    # Properly format feature importances\n",
    "    named_importances = list(zip(feature_names, feature_importance_values))\n",
    "    sorted_importances = sorted(named_importances, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Fit on full sample \n",
    "    if param_search:\n",
    "        estimator = model(**gridsearch_model.best_params_)\n",
    "    else:\n",
    "        estimator = model(**kwargs)\n",
    "        \n",
    "    estimator.fit(features, labels_smoothed)\n",
    "    \n",
    "    # Test on hold out set\n",
    "    out_of_sample_probs = estimator.predict_proba(holdout)\n",
    "    pos_probs = [p[1] for p in out_of_sample_probs]\n",
    "    predicted = [1 if y >= opt_thresh_ else 0 for y in pos_probs]\n",
    "\n",
    "    # Store values for later reporting/use in app\n",
    "    evals = {'precision':np.mean(scores['precision']), \n",
    "             'recall':np.mean(scores['recall']), \n",
    "             'accuracy':np.mean(scores['accuracy']), \n",
    "             'f1':np.mean(scores['f1']),\n",
    "             'probabilities':pos_probs,\n",
    "             'predictions':predicted,\n",
    "             'importances':sorted_importances\n",
    "             }\n",
    "    \n",
    "    # Report\n",
    "    print(\"Build, hyperparameter selection, and validation of {} took {:0.3f} seconds\\n\".format(label, time.time()-start))\n",
    "    print(\"Hyperparameters are as follows:\")\n",
    "    if param_search:\n",
    "        for key in gridsearch_model.best_params_.keys():\n",
    "            print(\"{}: {}\\n\".format(key, gridsearch_model.best_params_[key]))\n",
    "    print(\"Validation scores are as follows:\")\n",
    "    print(pd.DataFrame(scores).mean())\n",
    "    \n",
    "    # Output the evals dictionary\n",
    "    if export:\n",
    "        outpath = \"{}_{}.pickle\".format(label.tolower().replace(\" \", \"_\"), ticker.tolower())\n",
    "        with open(outpath, 'wb') as f:\n",
    "            pickle.dump(evals, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_sklearn_regressor(X, y, holdout, ticker, ema_gamma, valid_splits, model, label, param_search={}, export=False, \n",
    "                                **kwargs):\n",
    "    \n",
    "    start = time.time()\n",
    "    # Convert to NumPy arrays - store feature names\n",
    "    feature_names = X.columns.tolist()\n",
    "    features = np.array(X)\n",
    "    test_features = np.array(holdout)\n",
    "    returns = y.copy()\n",
    "    \n",
    "    # Compute EMA smoothing of target prior to constructing classes\n",
    "    EMA = 0\n",
    "    gamma_ = ema_gamma\n",
    "    for ti in range(len(returns)):\n",
    "        EMA = gamma_*returns[ti] + (1-gamma_)*EMA\n",
    "        returns[ti] = EMA  \n",
    "        \n",
    "    returns_smoothed = returns.copy() \n",
    "\n",
    "    # Set time-series, cross-validation indices\n",
    "    splits  = TimeSeriesSplit(n_splits=valid_splits)\n",
    "    search_splits = TimeSeriesSplit(n_splits=valid_splits).split(features)\n",
    "    \n",
    "    # Empty array for feature importances\n",
    "    feature_importance_values = np.zeros(len(feature_names))\n",
    "\n",
    "    # Empty array for out of fold validation predictions\n",
    "    out_of_fold = np.zeros(features.shape[0])\n",
    "    \n",
    "    # Empty array for test predictions\n",
    "    test_predictions = np.zeros(test_features.shape[0])\n",
    "    \n",
    "    # Dictionary of lists for recording validation and training scores\n",
    "    scores = {'mae':[], 'mse':[], 'r^2':[]}\n",
    "    \n",
    "    # Perform a grid-search on the provided parameters to determine best options\n",
    "    if param_search:\n",
    "        gridsearch = GridSearchCV(estimator=model(), \n",
    "                                  cv=search_splits,\n",
    "                                  param_grid=param_search)\n",
    "\n",
    "        # Fit to extract best parameters later\n",
    "        gridsearch_model = gridsearch.fit(features, returns_smoothed)\n",
    "\n",
    "    opt_thresholds = []\n",
    "    for train_indices, valid_indices in splits.split(features):\n",
    "        # Training data for the fold\n",
    "        train_features, train_returns = features[train_indices], returns_smoothed[train_indices]\n",
    "        #Validation data for the fold\n",
    "        valid_features, valid_returns = features[valid_indices], returns_smoothed[valid_indices]\n",
    "        \n",
    "        expected = valid_returns\n",
    "        \n",
    "        # Generate a model given the optimal parameters established in grid search\n",
    "        if param_search:\n",
    "            estimator = model(**gridsearch_model.best_params_)\n",
    "        else:\n",
    "            estimator = model(**kwargs)\n",
    "            \n",
    "        # Train the estimator\n",
    "        estimator.fit(train_features, train_returns)   \n",
    "        \n",
    "        # Generate predictions\n",
    "        predicted = estimator.predict(valid_features)\n",
    "\n",
    "        # Append scores to the tracker\n",
    "        scores['mae'].append(metrics.mean_absolute_error(expected, predicted))\n",
    "        scores['mse'].append(metrics.mean_squared_error(expected, predicted))\n",
    "        scores['r^2'].append(metrics.r2_score(expected, predicted))\n",
    "        \n",
    "        # Store variable importances \n",
    "        if model in [RandomForestClassifier, GradientBoostingClassifier]:\n",
    "            feature_importance_values += estimator.feature_importances_ / splits.n_splits\n",
    "            \n",
    "    # Properly format feature importances\n",
    "    named_importances = list(zip(feature_names, feature_importance_values))\n",
    "    sorted_importances = sorted(named_importances, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Fit on full sample \n",
    "    if param_search:\n",
    "        estimator = model(**gridsearch_model.best_params_)\n",
    "    else:\n",
    "        estimator = model(**kwargs)\n",
    "        \n",
    "    estimator.fit(features, returns_smoothed)\n",
    "    \n",
    "    # Test on hold out set\n",
    "    predicted = estimator.predict(holdout)    \n",
    "    \n",
    "    # Store values for later reporting/use in app\n",
    "    evals = {'mae':np.mean(scores['mae']), \n",
    "             'mse':np.mean(scores['mse']), \n",
    "             'r^2':np.mean(scores['r^2']),\n",
    "             'predictions':predicted,\n",
    "             'importances':sorted_importances\n",
    "             }\n",
    "    \n",
    "    # Report\n",
    "    print(\"Build, hyperparameter selection, and validation of {} took {:0.3f} seconds\\n\".format(label, time.time()-start))\n",
    "    print(\"Hyperparameters are as follows:\")\n",
    "    if param_search:\n",
    "        for key in gridsearch_model.best_params_.keys():\n",
    "            print(\"{}: {}\\n\".format(key, gridsearch_model.best_params_[key]))\n",
    "    print(\"Validation scores are as follows:\")\n",
    "    print(pd.DataFrame(scores).mean())\n",
    "    \n",
    "    # Output the evals dictionary\n",
    "    if export:\n",
    "        outpath = \"{}_{}.pickle\".format(label.tolower().replace(\" \", \"_\"), ticker.tolower())\n",
    "        with open(outpath, 'wb') as f:\n",
    "            pickle.dump(evals, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_lgbm_classifier(X, y, holdout, ticker, ema_gamma=1, valid_splits=12, export=False, valid_method=\"ts\", groups=None,\n",
    "                       labeled=False, param_search=True):\n",
    "    \n",
    "    # Convert to NumPy arrays - store feature names\n",
    "    feature_names = X.columns.tolist()\n",
    "    features = np.array(X)\n",
    "    test_features = np.array(holdout)\n",
    "    labels = y.copy()\n",
    "    \n",
    "    # Compute EMA smoothing of target prior to constructing classes\n",
    "    EMA = 0\n",
    "    gamma_ = ema_gamma\n",
    "    for ti in range(len(labels)):\n",
    "        EMA = gamma_*labels[ti] + (1-gamma_)*EMA\n",
    "        labels[ti] = EMA\n",
    "        \n",
    "    if labeled:\n",
    "        labels_smoothed = labels.copy()\n",
    "        orig_lables = labels.copy()\n",
    "    else:\n",
    "        # Compute EMA smoothing of target prior to constructing classes\n",
    "        EMA = 0\n",
    "        gamma_ = ema_gamma\n",
    "        for ti in range(len(labels)):\n",
    "            EMA = gamma_*labels[ti] + (1-gamma_)*EMA\n",
    "            labels[ti] = EMA  \n",
    "\n",
    "        labels_smoothed = np.where(labels > 0, 1, 0)    \n",
    "        orig_labels = np.where(y.copy() > 0, 1, 0)\n",
    "        print(\"\\n{} labels changed by smoothing.\".format(np.sum(labels_smoothed != orig_labels)))\n",
    "\n",
    "    # Compute some baselines\n",
    "    all_pos_acc = np.mean(np.ones(test_features.shape[0], dtype='int') == labels_smoothed)\n",
    "    random_walk_classes = np.array(pd.Series(labels_smoothed).shift(1).tolist()[1:], dtype=\"int\")\n",
    "    rw_acc = np.mean(random_walk_classes == np.array(labels_smoothed[1:], dtype=\"int\"))\n",
    "    print(\"Baseline accuracy is: {}%\".format(100*np.round(all_pos_acc, 2)))\n",
    "    print(\"RW accuracy is {}%\".format(100*np.round(rw_acc, 2)))\n",
    "    \n",
    "    ## pd.DataFrame(list(zip(labels_smoothed, y.copy()))).plot()\n",
    "\n",
    "    # Set time-series, cross-validation indices\n",
    "    if valid_method == \"panel\":\n",
    "        splits = PanelSplit(n_folds=valid_splits, groups=groups)\n",
    "    elif valid_method == \"ts\":\n",
    "        splits = TimeSeriesSplit(n_splits=valid_splits).split(X)\n",
    "        search_splits = TimeSeriesSplit(n_splits=valid_splits).split(X)\n",
    "    elif valid_method == \"kfold\":\n",
    "        splits = KFold(n_splits = valid_splits).split(X)\n",
    "\n",
    "    # Empty array for feature importances\n",
    "    feature_importance_values = np.zeros(len(feature_names))\n",
    "\n",
    "    # Empty array for out of fold validation predictions\n",
    "    out_of_fold = np.zeros(features.shape[0])\n",
    "    \n",
    "    # Empty array for test predictions\n",
    "    test_predictions = np.zeros(test_features.shape[0])\n",
    "\n",
    "    # Lists for recording validation and training scores\n",
    "    valid_scores = []\n",
    "    train_scores = []\n",
    "    valid_accs = []\n",
    "    train_accs = []\n",
    "    \n",
    "    # Perform a grid-search on the provided parameters to determine best options\n",
    "    if param_search:\n",
    "        print(\"Performing grid search for hyperparameter tuning\")\n",
    "        gridsearch = GridSearchCV(estimator=LGBMClassifier(boosting_type='gbdt',  objective='binary', num_boost_round=2000, learning_rate=0.01, metric='auc'), \n",
    "                                  cv=search_splits,\n",
    "                                  param_grid={'learning_rate':[0.1,0.01]})\n",
    "\n",
    "        # Fit to extract best parameters later\n",
    "        gridsearch_model = gridsearch.fit(features, labels_smoothed)\n",
    "        print(gridsearch_model)\n",
    "    \n",
    "    # Iterate through each fold\n",
    "    for train_indices, valid_indices in splits: \n",
    "\n",
    "        # Training data for the fold\n",
    "        train_features, train_labels = features[train_indices], labels_smoothed[train_indices]\n",
    "        #Validation data for the fold\n",
    "        valid_features, valid_labels = features[valid_indices], labels_smoothed[valid_indices]\n",
    "\n",
    "        # Create the bst\n",
    "        bst = LGBMClassifier(n_estimators=10000, objective = 'binary', \n",
    "                             class_weight = 'balanced', learning_rate = 0.01,\n",
    "                             max_bin = 50, num_leaves = 50, max_depth = 2,\n",
    "                             reg_alpha = 0.1, reg_lambda = 0.1, \n",
    "                             subsample = 0.8, random_state = 101,\n",
    "                             boosting = 'gbdt'\n",
    "                            )\n",
    "\n",
    "        # Train the bst\n",
    "        bst.fit(train_features, train_labels, eval_metric = ['auc', 'binary_error'],\n",
    "                eval_set = [(valid_features, valid_labels), (train_features, train_labels)],\n",
    "                eval_names = ['valid', 'train'],\n",
    "                early_stopping_rounds = 100, verbose = 0)\n",
    "\n",
    "        # Record the best iteration\n",
    "        best_iteration = bst.best_iteration_\n",
    "\n",
    "        # Record the feature importances\n",
    "        feature_importance_values += bst.feature_importances_ / valid_splits\n",
    "        \n",
    "        # Make predictions\n",
    "        test_predictions += bst.predict_proba(test_features, num_iteration = best_iteration)[:, 1] / valid_splits\n",
    "\n",
    "        # Record the out of fold predictions\n",
    "        out_of_fold[valid_indices] = bst.predict_proba(valid_features, num_iteration = best_iteration)[:, 1]\n",
    "\n",
    "        # Record the best score\n",
    "        valid_score = bst.best_score_['valid']['auc']\n",
    "        train_score = bst.best_score_['train']['auc']\n",
    "        valid_acc = bst.best_score_['valid']['binary_error']\n",
    "        train_acc = bst.best_score_['train']['binary_error']\n",
    "        \n",
    "        valid_scores.append(valid_score)\n",
    "        train_scores.append(train_score)\n",
    "        valid_accs.append(valid_acc)\n",
    "        train_accs.append(train_acc)\n",
    "        \n",
    "    # Properly format feature importances\n",
    "    named_importances = list(zip(feature_names, feature_importance_values))\n",
    "    sorted_importances = sorted(named_importances, key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "    # Set up an exportable dictionary with results from the model\n",
    "    results = {\n",
    "        'train_auc':train_scores,\n",
    "        'train_acc':train_acc,\n",
    "        'validation_auc':valid_scores,\n",
    "        'validation_accuracy':valid_acc,\n",
    "        'valid_preds':out_of_fold,\n",
    "        'feature_importances':sorted_importances,\n",
    "        'test_predictions':test_predictions\n",
    "    }\n",
    "    \n",
    "    # Output the results dictionary\n",
    "    if export:\n",
    "        outpath = \"lgbc_{}.pickle\".format(ticker.tolower())\n",
    "        with open(outpath, 'wb') as f:\n",
    "            pickle.dump(results, f)\n",
    "    \n",
    "    print(\"Average AUC across {} splits: {}\".format(valid_splits, np.mean(valid_scores)))\n",
    "    print(\"Average accuracy across {} splits: {}%\".format(valid_splits, 100*np.round((1-np.mean(valid_acc)),2)))\n",
    "    for i in range(6):\n",
    "        print(sorted_importances[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_lgbm_regressor(X, y, holdout, ticker, ema_gamma=1, valid_splits=12, export=False):\n",
    "    \n",
    "    # Convert to NumPy arrays - store feature names\n",
    "    feature_names = X.columns.tolist()\n",
    "    features = np.array(X)\n",
    "    test_features = np.array(holdout)\n",
    "    returns = y.copy()\n",
    "    \n",
    "    # Compute EMA smoothing of target prior to constructing classes\n",
    "    EMA = 0\n",
    "    gamma_ = ema_gamma\n",
    "    for ti in range(len(returns)):\n",
    "        EMA = gamma_*returns[ti] + (1-gamma_)*EMA\n",
    "        returns[ti] = EMA  \n",
    "        \n",
    "    returns_smoothed = returns.copy()\n",
    "\n",
    "    # Generate splits\n",
    "    splits = TimeSeriesSplit(n_splits=valid_splits)\n",
    "\n",
    "    # Empty array for feature importances\n",
    "    feature_importance_values = np.zeros(len(feature_names))\n",
    "\n",
    "    # Empty array for out of fold validation predictions\n",
    "    out_of_fold = np.zeros(features.shape[0])\n",
    "    \n",
    "    # Empty array for test predictions\n",
    "    test_predictions = np.zeros(test_features.shape[0])\n",
    "\n",
    "    # Lists for recording validation and training scores\n",
    "    valid_scores = []\n",
    "    train_scores = []\n",
    "    \n",
    "    # Iterate through each fold\n",
    "    for train_indices, valid_indices in splits.split(X): \n",
    "\n",
    "        # Training data for the fold\n",
    "        train_features, train_returns = features[train_indices], returns_smoothed[train_indices]\n",
    "        #Validation data for the fold\n",
    "        valid_features, valid_returns = features[valid_indices], returns_smoothed[valid_indices]\n",
    "\n",
    "        # Create the bst\n",
    "        bst = LGBMRegressor(n_estimators=10000, objective = 'regression', \n",
    "                            learning_rate = 0.01,\n",
    "                            max_bin = 25, num_leaves = 25, max_depth = 1,\n",
    "                            reg_alpha = 0.1, reg_lambda = 0.1, \n",
    "                            subsample = 0.8, random_state = 101,\n",
    "                            boosting = 'gbdt'\n",
    "                            )\n",
    "\n",
    "        # Train the bst\n",
    "        bst.fit(train_features, train_returns, eval_metric = ['l1', 'l2'],\n",
    "                eval_set = [(valid_features, valid_returns), (train_features, train_returns)],\n",
    "                eval_names = ['valid', 'train'],\n",
    "                early_stopping_rounds = 100, verbose = 0)\n",
    "\n",
    "        # Record the best iteration\n",
    "        best_iteration = bst.best_iteration_\n",
    "\n",
    "        # Record the feature importances\n",
    "        feature_importance_values += bst.feature_importances_ / splits.n_splits\n",
    "        \n",
    "        # Make predictions\n",
    "        test_predictions += bst.predict(test_features, num_iteration = best_iteration) / splits.n_splits\n",
    "\n",
    "        # Record the out of fold predictions\n",
    "        out_of_fold[valid_indices] = bst.predict(valid_features, num_iteration = best_iteration)\n",
    "\n",
    "        # Record the best score\n",
    "        valid_score = bst.best_score_['valid']['l1']\n",
    "        train_score = bst.best_score_['train']['l1']\n",
    "        \n",
    "        valid_scores.append(valid_score)\n",
    "        train_scores.append(train_score)\n",
    "        \n",
    "    # Properly format feature importances\n",
    "    named_importances = list(zip(feature_names, feature_importance_values))\n",
    "    sorted_importances = sorted(named_importances, key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "    # Set up an exportable dictionary with results from the model\n",
    "    results = {\n",
    "        'train_auc':train_scores,\n",
    "        'validation_auc':valid_scores,\n",
    "        'valid_preds':out_of_fold,\n",
    "        'feature_importances':sorted_importances,\n",
    "        'test_predictions':test_predictions\n",
    "    }\n",
    "    \n",
    "    # Output the results dictionary\n",
    "    if export:\n",
    "        outpath = \"lgbr_{}.pickle\".format(ticker.tolower())\n",
    "        with open(outpath, 'wb') as f:\n",
    "            pickle.dump(results, f)\n",
    "    \n",
    "    print(\"\\nAverage MAE across {} splits: {}\".format(valid_splits, np.mean(valid_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Panel-level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that there are bound to be a number of systemic considerations that impact the price of a stock at any given point in time, it is prudent to perform and evaluate predictions across the panel of S&P 500 stocks in our sample, which will capture potential linkages between different stocks, and allow us to explore the possibility of using features generated from clustering to group like stocks in the panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create copies for panel-level regressions\n",
    "X_p = X.copy(deep=True)\n",
    "y_p = target_raw.copy()\n",
    "\n",
    "# Indexes of hold-out test data (the 21 days of data preceding the present day)\n",
    "test_idx = np.where(np.isnan(y_p))[0].tolist()\n",
    "\n",
    "# In order to ensure grouping is done properly, remove this data from a ticker-identification set as well\n",
    "ticker_locs = train['ticker'].drop(train.index[test_idx]).reset_index().drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PanelSplit(n_folds, groups, grouping_var='ticker'):\n",
    "    \"\"\"\n",
    "    Function to generate time series splits of a panel, provided\n",
    "    a number of folds, and an indexable dataframe to create groups.\n",
    "    Returns a generator object for compliance with sci-kit learn API.\n",
    "    \"\"\"\n",
    "    by_ticker_index = (groups.groupby(grouping_var)\n",
    "                       .apply(lambda x: x.reset_index(drop=True))\n",
    "                       .drop(grouping_var, axis=1)\n",
    "                       .reset_index()\n",
    "                       .rename({'level_1':'tsidx'}, axis=1)\n",
    "                       )\n",
    "    \n",
    "    ticker_range = by_ticker_index['tsidx'].unique().tolist()\n",
    "    ticker_range = sorted(ticker_range)\n",
    "    \n",
    "    splits = TimeSeriesSplit(n_splits=n_folds)\n",
    "    \n",
    "    for train_indices, valid_indices in splits.split(ticker_range):\n",
    "        panel_train_indices = by_ticker_index[by_ticker_index['tsidx'].isin(train_indices)].index.tolist()\n",
    "        panel_valid_indices = by_ticker_index[by_ticker_index['tsidx'].isin(valid_indices)].index.tolist()\n",
    "        yield panel_train_indices, panel_valid_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WindowSplit(window, groups, panel):    \n",
    "    \"\"\"\n",
    "    Function to generate windowed time series splits of a panel, provided\n",
    "    a number of folds, and an indexable dataframe to create groups.\n",
    "    Returns a generator object for compliance with sci-kit learn API.\n",
    "    \"\"\"    \n",
    "    wparams = window.split(':')\n",
    "    wtrain = int(wparams[0])\n",
    "    wvalid = int(wparams[1])\n",
    "    witer = int(wparams[2])\n",
    "    \n",
    "    by_ticker_index = (groups.groupby('ticker')\n",
    "                       .apply(lambda x: x.reset_index(drop=True))\n",
    "                       .drop('ticker', axis=1)\n",
    "                       .reset_index()\n",
    "                       .rename({'level_1':'tsidx'}, axis=1)\n",
    "                       )\n",
    "    \n",
    "    ticker_range = by_ticker_index['tsidx'].unique().tolist()\n",
    "    ticker_range = sorted(ticker_range)\n",
    "    \n",
    "    stop = 0\n",
    "    start = (max(ticker_range) % witer) + 1\n",
    "    if panel:\n",
    "        while stop < max(ticker_range):\n",
    "            train_indices = np.arange(start, start + wtrain).tolist()\n",
    "            valid_indices = np.arange(start + wtrain, start + wtrain + wvalid).tolist()\n",
    "            stop = max(valid_indices)\n",
    "            start += witer\n",
    "            yield train_indices, valid_indices\n",
    "    else:\n",
    "        while stop < max(ticker_range):\n",
    "            train_indices = np.arange(start, start + wtrain).tolist()\n",
    "            valid_indices = np.arange(start + wtrain, start + wtrain + wvalid).tolist()\n",
    "            panel_train_indices = by_ticker_index[by_ticker_index['tsidx'].isin(train_indices)].index.tolist()\n",
    "            panel_valid_indices = by_ticker_index[by_ticker_index['tsidx'].isin(valid_indices)].index.tolist()\n",
    "            stop = max(valid_indices)\n",
    "            start += witer\n",
    "            yield panel_train_indices, panel_valid_indices      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple feature-scaling - for now, replace missings with 0 (i.e. the mean of a normalized feature) within days\n",
    "X_p = X_p.groupby(['Year', 'Month', 'Day']).apply(lambda x: (x - np.mean(x))/np.std(x)).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove hold-out test data\n",
    "y_p = np.delete(y_p, test_idx)\n",
    "X_p_holdout = X_p.loc[X_p.index[test_idx]]\n",
    "X_p = X_p.drop(X_p.index[test_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_p_smoothed = np.zeros(y_p.shape[0])\n",
    "for t in tickers:\n",
    "    idx = ticker_locs.loc[ticker_locs['ticker'] == t].index.tolist()\n",
    "    y_to_smooth = y_p[idx]\n",
    "    \n",
    "    # Compute EMA smoothing of target within ticker\n",
    "    EMA = 0\n",
    "    gamma_ = 1\n",
    "    for ti in range(len(y_to_smooth)):\n",
    "        EMA = gamma_*y_to_smooth[ti] + (1-gamma_)*EMA\n",
    "        y_to_smooth[ti] = EMA\n",
    "        \n",
    "    y_p_smoothed[idx] = y_to_smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0 labels changed by smoothing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:33: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:33: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy is: 0.0%\n",
      "RW accuracy is 49.0%\n",
      "Average AUC across 2 splits: 0.5108652978840644\n",
      "Average accuracy across 2 splits: 51.0%\n",
      "('Pct_Change_Class', 313.5)\n",
      "('Volatility', 70.5)\n",
      "('Rolling_Yearly_Mean_Positive_Days', 59.0)\n",
      "('Yearly_Return_Rank', 52.5)\n",
      "('roa', 42.5)\n",
      "('debt_ratio', 25.5)\n"
     ]
    }
   ],
   "source": [
    "# Fit and evaluate - gamma MUST be 1 here\n",
    "fit_lgbm_classifier(X_p, y_p_smoothed, X_p_holdout, ticker=\"\", ema_gamma=1, valid_splits=2, export=False, \n",
    "                    valid_method = 'panel', groups = ticker_locs)\n",
    "## fit_sklearn_classifier(X_p, y_p_smoothed, X_p_holdout, ticker=\"\", ema_gamma=1, valid_splits=12, model=GradientBoostingClassifier,\n",
    "##                        label='kNN Classifier', param_search = {}, export=False, \n",
    "##                        panel = True, groups=ticker_locs, n_estimators = 1000, learning_rate = 0.01, max_depth=2\n",
    "##                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ticker-level "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the heart of this analysis is a time-series prediction problem. As such, it is prudent to explore running models for each individual stock. We can envision averaging the results of both modeling approaches to incorporate the contribution of both into a final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0 labels changed by smoothing.\n",
      "Baseline accuracy is: 0.0%\n",
      "RW accuracy is 90.0%\n",
      "Performing grid search for hyperparameter tuning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:33: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(cv=<generator object TimeSeriesSplit.split at 0x000001F280C53F48>,\n",
      "       error_score='raise-deprecating',\n",
      "       estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "        importance_type='split', learning_rate=0.01, max_depth=-1,\n",
      "        metric='auc', min_child_samples=20, min_child_weight=0.001,\n",
      "        min_split_gain=0.0, n_estimators=100, n_jobs=-1,\n",
      "        num_boost_round=2000, num_leaves=31, objective='binary',\n",
      "        random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
      "        subsample=1.0, subsample_for_bin=200000, subsample_freq=0),\n",
      "       fit_params=None, iid='warn', n_jobs=None,\n",
      "       param_grid={'learning_rate': [0.1, 0.01]}, pre_dispatch='2*n_jobs',\n",
      "       refit=True, return_train_score='warn', scoring=None, verbose=0)\n",
      "Average AUC across 12 splits: 0.5584719377954542\n",
      "Average accuracy across 12 splits: 70.0%\n",
      "('Rolling_Yearly_Mean_Price', 7.333333333333333)\n",
      "('pe_ratio', 3.4166666666666674)\n",
      "('close_l5', 2.1666666666666665)\n",
      "('roa', 2.1666666666666665)\n",
      "('Year', 2.0833333333333335)\n",
      "('Close', 2.083333333333333)\n",
      "\n",
      "0 labels changed by smoothing.\n",
      "Baseline accuracy is: 0.0%\n",
      "RW accuracy is 92.0%\n",
      "Performing grid search for hyperparameter tuning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:33: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(cv=<generator object TimeSeriesSplit.split at 0x000001F280415D68>,\n",
      "       error_score='raise-deprecating',\n",
      "       estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "        importance_type='split', learning_rate=0.01, max_depth=-1,\n",
      "        metric='auc', min_child_samples=20, min_child_weight=0.001,\n",
      "        min_split_gain=0.0, n_estimators=100, n_jobs=-1,\n",
      "        num_boost_round=2000, num_leaves=31, objective='binary',\n",
      "        random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
      "        subsample=1.0, subsample_for_bin=200000, subsample_freq=0),\n",
      "       fit_params=None, iid='warn', n_jobs=None,\n",
      "       param_grid={'learning_rate': [0.1, 0.01]}, pre_dispatch='2*n_jobs',\n",
      "       refit=True, return_train_score='warn', scoring=None, verbose=0)\n",
      "Average AUC across 12 splits: 0.5511166240292067\n",
      "Average accuracy across 12 splits: 35.0%\n",
      "('Rolling_Yearly_Mean_Price', 5.999999999999999)\n",
      "('pe_ratio', 4.333333333333333)\n",
      "('RSI', 2.9166666666666665)\n",
      "('Rolling_Yearly_Mean_Positive_Days', 2.666666666666667)\n",
      "('Rolling_Monthly_Mean_Price', 2.5833333333333335)\n",
      "('roa', 2.2500000000000004)\n",
      "\n",
      "0 labels changed by smoothing.\n",
      "Baseline accuracy is: 0.0%\n",
      "RW accuracy is 90.0%\n",
      "Performing grid search for hyperparameter tuning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:33: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(cv=<generator object TimeSeriesSplit.split at 0x000001F280C53840>,\n",
      "       error_score='raise-deprecating',\n",
      "       estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "        importance_type='split', learning_rate=0.01, max_depth=-1,\n",
      "        metric='auc', min_child_samples=20, min_child_weight=0.001,\n",
      "        min_split_gain=0.0, n_estimators=100, n_jobs=-1,\n",
      "        num_boost_round=2000, num_leaves=31, objective='binary',\n",
      "        random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
      "        subsample=1.0, subsample_for_bin=200000, subsample_freq=0),\n",
      "       fit_params=None, iid='warn', n_jobs=None,\n",
      "       param_grid={'learning_rate': [0.1, 0.01]}, pre_dispatch='2*n_jobs',\n",
      "       refit=True, return_train_score='warn', scoring=None, verbose=0)\n",
      "Average AUC across 12 splits: 0.5561991166300916\n",
      "Average accuracy across 12 splits: 72.0%\n",
      "('Pct_Change_Yearly', 5.666666666666667)\n",
      "('Yearly_Return_Rank', 4.083333333333334)\n",
      "('Rolling_Monthly_Mean_Positive_Days', 3.583333333333334)\n",
      "('Monthly_Return_Rank', 3.0833333333333335)\n",
      "('roa', 3.0833333333333335)\n",
      "('Rolling_Yearly_Mean_Price', 2.916666666666667)\n",
      "\n",
      "0 labels changed by smoothing.\n",
      "Baseline accuracy is: 0.0%\n",
      "RW accuracy is 90.0%\n",
      "Performing grid search for hyperparameter tuning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:33: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(cv=<generator object TimeSeriesSplit.split at 0x000001F280415CF0>,\n",
      "       error_score='raise-deprecating',\n",
      "       estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "        importance_type='split', learning_rate=0.01, max_depth=-1,\n",
      "        metric='auc', min_child_samples=20, min_child_weight=0.001,\n",
      "        min_split_gain=0.0, n_estimators=100, n_jobs=-1,\n",
      "        num_boost_round=2000, num_leaves=31, objective='binary',\n",
      "        random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
      "        subsample=1.0, subsample_for_bin=200000, subsample_freq=0),\n",
      "       fit_params=None, iid='warn', n_jobs=None,\n",
      "       param_grid={'learning_rate': [0.1, 0.01]}, pre_dispatch='2*n_jobs',\n",
      "       refit=True, return_train_score='warn', scoring=None, verbose=0)\n",
      "Average AUC across 12 splits: 0.5076498550026333\n",
      "Average accuracy across 12 splits: 48.0%\n",
      "('Yearly_Return_Rank', 0.6666666666666666)\n",
      "('debt_ratio', 0.49999999999999994)\n",
      "('Low', 0.3333333333333333)\n",
      "('Close', 0.25)\n",
      "('Rolling_Yearly_Mean_Price', 0.25)\n",
      "('Pct_Change_Yearly', 0.16666666666666666)\n",
      "\n",
      "0 labels changed by smoothing.\n",
      "Baseline accuracy is: 0.0%\n",
      "RW accuracy is 91.0%\n",
      "Performing grid search for hyperparameter tuning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:33: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\jared\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(cv=<generator object TimeSeriesSplit.split at 0x000001F280C53DE0>,\n",
      "       error_score='raise-deprecating',\n",
      "       estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "        importance_type='split', learning_rate=0.01, max_depth=-1,\n",
      "        metric='auc', min_child_samples=20, min_child_weight=0.001,\n",
      "        min_split_gain=0.0, n_estimators=100, n_jobs=-1,\n",
      "        num_boost_round=2000, num_leaves=31, objective='binary',\n",
      "        random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
      "        subsample=1.0, subsample_for_bin=200000, subsample_freq=0),\n",
      "       fit_params=None, iid='warn', n_jobs=None,\n",
      "       param_grid={'learning_rate': [0.1, 0.01]}, pre_dispatch='2*n_jobs',\n",
      "       refit=True, return_train_score='warn', scoring=None, verbose=0)\n",
      "Average AUC across 12 splits: 0.5724479414217507\n",
      "Average accuracy across 12 splits: 67.0%\n",
      "('Yearly_Return_Rank', 3.750000000000001)\n",
      "('Week', 3.7500000000000004)\n",
      "('Dayofyear', 2.750000000000001)\n",
      "('High', 1.6666666666666665)\n",
      "('roa', 1.6666666666666665)\n",
      "('RSI', 1.4166666666666665)\n"
     ]
    }
   ],
   "source": [
    "for i, t in enumerate(tickers[:10]):\n",
    "    \n",
    "    # Pull only feature/target data for the relevant stocker\n",
    "    X_t = X.loc[train['ticker'] == t,:]\n",
    "    y_t = np.array(target_raw)[train['ticker'] == t]\n",
    "    \n",
    "    # Indexes of hold-out test data (the 21 days of data preceding the present day)\n",
    "    test_idx = np.where(np.isnan(y_t))[0].tolist()\n",
    "    \n",
    "    # Simple feature-scaling - for now, replace missings with 0 (i.e. the mean of a normalized feature)\n",
    "    X_t = X_t.apply(lambda x: (x - np.mean(x))/np.std(x)).fillna(0)\n",
    "    \n",
    "    # Remove hold-out test data\n",
    "    y_t = np.delete(y_t, test_idx)\n",
    "    X_t_holdout = X_t.loc[X_t.index[test_idx]]\n",
    "    X_t = X_t.drop(X_t.index[test_idx])\n",
    "    \n",
    "    # Fit and evaluate\n",
    "    fit_lgbm_classifier(X_t, y_t, X_t_holdout, ticker=t, ema_gamma=1, valid_splits=12, export=False, valid_method='ts',\n",
    "                        labeled = False)\n",
    "    ## fit_lgbm_regressor(X_t, y_t, X_t_holdout, ticker=t, ema_gamma=1, valid_splits=12, export=False)\n",
    "    ## fit_sklearn_classifier(X_t, y_t, X_t_holdout, ticker=t, ema_gamma=1, valid_splits=12, model=GradientBoostingClassifier,\n",
    "    ##                        label='kNN Classifier', param_search = {}, export=False, n_estimators = 1000, \n",
    "    ##                        learning_rate = 0.01, max_depth = 1\n",
    "    ##                      )\n",
    "    ## fit_sklearn_regressor(X_t, y_t, X_t_holdout, ticker=t, ema_gamma=0.1, valid_splits=12, model=KNeighborsRegressor,\n",
    "    ##                        label='kNN Regressor', param_search = {}, export=False, n_jobs=-1\n",
    "    ##                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Machine\n",
      "Accuracy: 0.6115288220551378\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.02      0.04       154\n",
      "           1       0.61      0.98      0.76       245\n",
      "\n",
      "   micro avg       0.61      0.61      0.61       399\n",
      "   macro avg       0.52      0.50      0.40       399\n",
      "weighted avg       0.54      0.61      0.48       399\n",
      "\n",
      "Gradient Boosting Machine\n",
      "Accuracy: 0.5839598997493735\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.05      0.10       169\n",
      "           1       0.58      0.97      0.73       230\n",
      "\n",
      "   micro avg       0.58      0.58      0.58       399\n",
      "   macro avg       0.59      0.51      0.41       399\n",
      "weighted avg       0.59      0.58      0.46       399\n",
      "\n",
      "Gradient Boosting Machine\n",
      "Accuracy: 0.5714285714285714\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.58      0.49       142\n",
      "           1       0.71      0.56      0.63       257\n",
      "\n",
      "   micro avg       0.57      0.57      0.57       399\n",
      "   macro avg       0.57      0.57      0.56       399\n",
      "weighted avg       0.61      0.57      0.58       399\n",
      "\n",
      "Gradient Boosting Machine\n",
      "Accuracy: 0.5839598997493735\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.02      0.05       164\n",
      "           1       0.59      0.97      0.73       235\n",
      "\n",
      "   micro avg       0.58      0.58      0.58       399\n",
      "   macro avg       0.49      0.50      0.39       399\n",
      "weighted avg       0.51      0.58      0.45       399\n",
      "\n",
      "Gradient Boosting Machine\n",
      "Accuracy: 0.48621553884711777\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.96      0.60       158\n",
      "           1       0.86      0.18      0.30       241\n",
      "\n",
      "   micro avg       0.49      0.49      0.49       399\n",
      "   macro avg       0.65      0.57      0.45       399\n",
      "weighted avg       0.69      0.49      0.41       399\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "for i, t in enumerate(tickers[:10]):\n",
    "    \n",
    "    # Pull only feature/target data for the relevant stocker\n",
    "    X_t = X.loc[train['ticker'] == t,:]\n",
    "    y_t = np.array(target_up)[train['ticker'] == t]\n",
    "    \n",
    "    # Indexes of hold-out test data (the 21 days of data preceding the present day)\n",
    "    test_idx = np.where(np.isnan(y_t))[0].tolist()\n",
    "    \n",
    "    # Simple feature-scaling - for now, replace missings with 0 (i.e. the mean of a normalized feature)\n",
    "    #X_t = X_t.apply(lambda x: (x - np.mean(x))/np.std(x)).fillna(0)\n",
    "    X_t = X_t.fillna(0)\n",
    "    \n",
    "    # Remove hold-out test data\n",
    "    y_t = np.delete(y_t, test_idx)\n",
    "    X_t_holdout = X_t.loc[X_t.index[test_idx]]\n",
    "    X_t = X_t.drop(X_t.index[test_idx])\n",
    "    \n",
    "    y_t = np.where(y_t > 0, 1, 0)\n",
    "    \n",
    "    split_ = int(X_t.shape[0] * 0.8)\n",
    "    X_train = X_t.iloc[:split_,:]\n",
    "    X_test = X_t.iloc[split_:,:]\n",
    "    y_train = y_t[:split_]\n",
    "    y_test = y_t[split_:]\n",
    "    \n",
    "    #X_train, X_test, y_train, y_test = train_test_split(X_t, y_t, test_size=0.2)\n",
    "    model = GradientBoostingClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    expected = y_test\n",
    "    probs = model.predict_proba(X_test)\n",
    "    pos_probs = [p[1] for p in probs]\n",
    "    predicted = np.where(np.array(pos_probs) >= 0.3, 1,0).tolist()\n",
    "\n",
    "    print('Gradient Boosting Machine')\n",
    "    print(\"Accuracy:\",metrics.accuracy_score(expected, predicted))\n",
    "    print()\n",
    "    print(classification_report(expected, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
